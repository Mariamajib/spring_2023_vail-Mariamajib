{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MySureStart/spring_2023_vail-Mariamajib/blob/main/Day_05/Introduction_to_Regression_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 5 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "outputId": "dc518257-54d3-4631-a715-bc45773efefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "outputId": "c663b16f-38ed-4d2f-d1c3-ca29d3e27162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 6s 23ms/step - loss: 575.2585 - mse: 575.2585 - val_loss: 478.3061 - val_mse: 478.3061\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 549.3928 - mse: 549.3928 - val_loss: 452.9337 - val_mse: 452.9337\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 514.8531 - mse: 514.8531 - val_loss: 416.7077 - val_mse: 416.7077\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 465.4367 - mse: 465.4367 - val_loss: 364.7026 - val_mse: 364.7026\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 396.1786 - mse: 396.1786 - val_loss: 295.6222 - val_mse: 295.6222\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 309.3314 - mse: 309.3314 - val_loss: 208.6593 - val_mse: 208.6593\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 210.9796 - mse: 210.9796 - val_loss: 126.7357 - val_mse: 126.7357\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 137.0700 - mse: 137.0700 - val_loss: 79.9966 - val_mse: 79.9966\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 98.5859 - mse: 98.5859 - val_loss: 61.2797 - val_mse: 61.2797\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 75.6179 - mse: 75.6179 - val_loss: 47.4054 - val_mse: 47.4054\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 58.5774 - mse: 58.5774 - val_loss: 37.4779 - val_mse: 37.4779\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 46.8398 - mse: 46.8398 - val_loss: 30.7219 - val_mse: 30.7219\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 38.8451 - mse: 38.8451 - val_loss: 25.8951 - val_mse: 25.8951\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 33.7876 - mse: 33.7876 - val_loss: 22.0417 - val_mse: 22.0417\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 29.8809 - mse: 29.8809 - val_loss: 19.2550 - val_mse: 19.2550\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 27.0824 - mse: 27.0824 - val_loss: 17.1479 - val_mse: 17.1479\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 24.9495 - mse: 24.9495 - val_loss: 15.5294 - val_mse: 15.5294\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 23.2371 - mse: 23.2371 - val_loss: 14.5248 - val_mse: 14.5248\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 22.0441 - mse: 22.0441 - val_loss: 13.5572 - val_mse: 13.5572\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20.9535 - mse: 20.9535 - val_loss: 12.7721 - val_mse: 12.7721\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20.1419 - mse: 20.1419 - val_loss: 12.1212 - val_mse: 12.1212\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19.6391 - mse: 19.6391 - val_loss: 11.7202 - val_mse: 11.7202\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 18.8082 - mse: 18.8082 - val_loss: 11.3149 - val_mse: 11.3149\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18.3561 - mse: 18.3561 - val_loss: 11.1433 - val_mse: 11.1433\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.8795 - mse: 17.8795 - val_loss: 10.9828 - val_mse: 10.9828\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 17.5450 - mse: 17.5450 - val_loss: 11.1314 - val_mse: 11.1314\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.1935 - mse: 17.1935 - val_loss: 11.0183 - val_mse: 11.0183\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.9219 - mse: 16.9219 - val_loss: 10.9868 - val_mse: 10.9868\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 16.6059 - mse: 16.6059 - val_loss: 10.7856 - val_mse: 10.7856\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.4205 - mse: 16.4205 - val_loss: 10.7158 - val_mse: 10.7158\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 16.1307 - mse: 16.1307 - val_loss: 10.4770 - val_mse: 10.4770\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15.9409 - mse: 15.9409 - val_loss: 10.5961 - val_mse: 10.5961\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.6893 - mse: 15.6893 - val_loss: 10.4504 - val_mse: 10.4504\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15.5670 - mse: 15.5670 - val_loss: 10.3189 - val_mse: 10.3189\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.3220 - mse: 15.3220 - val_loss: 10.1108 - val_mse: 10.1108\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.1586 - mse: 15.1586 - val_loss: 10.0478 - val_mse: 10.0478\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14.9778 - mse: 14.9778 - val_loss: 10.0095 - val_mse: 10.0095\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.8144 - mse: 14.8144 - val_loss: 10.0589 - val_mse: 10.0589\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.6232 - mse: 14.6232 - val_loss: 10.0507 - val_mse: 10.0507\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.5361 - mse: 14.5361 - val_loss: 9.9709 - val_mse: 9.9709\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.4805 - mse: 14.4805 - val_loss: 10.2232 - val_mse: 10.2232\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14.2586 - mse: 14.2586 - val_loss: 9.8182 - val_mse: 9.8182\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.0480 - mse: 14.0480 - val_loss: 9.8022 - val_mse: 9.8022\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.0450 - mse: 14.0450 - val_loss: 10.2894 - val_mse: 10.2894\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13.8422 - mse: 13.8422 - val_loss: 9.6597 - val_mse: 9.6597\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13.7413 - mse: 13.7413 - val_loss: 9.6403 - val_mse: 9.6403\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13.5637 - mse: 13.5637 - val_loss: 9.8909 - val_mse: 9.8909\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13.4099 - mse: 13.4099 - val_loss: 9.6097 - val_mse: 9.6097\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13.2987 - mse: 13.2987 - val_loss: 9.4504 - val_mse: 9.4504\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 13.2134 - mse: 13.2134 - val_loss: 9.5082 - val_mse: 9.5082\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 13.1299 - mse: 13.1299 - val_loss: 9.4396 - val_mse: 9.4396\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13.1065 - mse: 13.1065 - val_loss: 9.2114 - val_mse: 9.2114\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.9087 - mse: 12.9087 - val_loss: 9.3161 - val_mse: 9.3161\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 12.8116 - mse: 12.8116 - val_loss: 9.4080 - val_mse: 9.4080\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 12.7465 - mse: 12.7465 - val_loss: 9.3716 - val_mse: 9.3716\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.6148 - mse: 12.6148 - val_loss: 9.2328 - val_mse: 9.2328\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.5763 - mse: 12.5763 - val_loss: 9.0879 - val_mse: 9.0879\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 12.4334 - mse: 12.4334 - val_loss: 9.1196 - val_mse: 9.1196\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.3474 - mse: 12.3474 - val_loss: 9.2867 - val_mse: 9.2867\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.3293 - mse: 12.3293 - val_loss: 9.6247 - val_mse: 9.6247\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12.2442 - mse: 12.2442 - val_loss: 9.4772 - val_mse: 9.4772\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.2144 - mse: 12.2144 - val_loss: 9.3686 - val_mse: 9.3686\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 12.0156 - mse: 12.0156 - val_loss: 9.4735 - val_mse: 9.4735\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.0354 - mse: 12.0354 - val_loss: 9.0940 - val_mse: 9.0940\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 11.9461 - mse: 11.9461 - val_loss: 9.0721 - val_mse: 9.0721\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.8490 - mse: 11.8490 - val_loss: 8.8113 - val_mse: 8.8113\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.8198 - mse: 11.8198 - val_loss: 8.6506 - val_mse: 8.6506\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.6684 - mse: 11.6684 - val_loss: 8.8292 - val_mse: 8.8292\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.7485 - mse: 11.7485 - val_loss: 8.7224 - val_mse: 8.7224\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.5075 - mse: 11.5075 - val_loss: 8.7257 - val_mse: 8.7257\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.4988 - mse: 11.4988 - val_loss: 8.6657 - val_mse: 8.6657\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11.5804 - mse: 11.5804 - val_loss: 8.5839 - val_mse: 8.5839\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.4712 - mse: 11.4712 - val_loss: 8.3243 - val_mse: 8.3243\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.3425 - mse: 11.3425 - val_loss: 8.4369 - val_mse: 8.4369\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.2908 - mse: 11.2908 - val_loss: 8.4630 - val_mse: 8.4630\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.1991 - mse: 11.1991 - val_loss: 8.4902 - val_mse: 8.4902\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.1719 - mse: 11.1719 - val_loss: 8.5103 - val_mse: 8.5103\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.0651 - mse: 11.0651 - val_loss: 8.4640 - val_mse: 8.4640\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.0550 - mse: 11.0550 - val_loss: 8.4190 - val_mse: 8.4190\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.9991 - mse: 10.9991 - val_loss: 8.4887 - val_mse: 8.4887\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.9952 - mse: 10.9952 - val_loss: 8.4219 - val_mse: 8.4219\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.8550 - mse: 10.8550 - val_loss: 8.3441 - val_mse: 8.3441\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.8334 - mse: 10.8334 - val_loss: 8.2715 - val_mse: 8.2715\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.7440 - mse: 10.7440 - val_loss: 8.2592 - val_mse: 8.2592\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.7662 - mse: 10.7662 - val_loss: 8.3157 - val_mse: 8.3157\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.6425 - mse: 10.6425 - val_loss: 8.1473 - val_mse: 8.1473\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.5830 - mse: 10.5830 - val_loss: 8.2139 - val_mse: 8.2139\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.5419 - mse: 10.5419 - val_loss: 8.1812 - val_mse: 8.1812\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.5423 - mse: 10.5423 - val_loss: 8.1844 - val_mse: 8.1844\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.4857 - mse: 10.4857 - val_loss: 8.0014 - val_mse: 8.0014\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.5435 - mse: 10.5435 - val_loss: 8.1515 - val_mse: 8.1515\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.5210 - mse: 10.5210 - val_loss: 7.9625 - val_mse: 7.9625\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.3826 - mse: 10.3826 - val_loss: 8.2493 - val_mse: 8.2493\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.3099 - mse: 10.3099 - val_loss: 8.0060 - val_mse: 8.0060\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.2828 - mse: 10.2828 - val_loss: 8.0306 - val_mse: 8.0306\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.2619 - mse: 10.2619 - val_loss: 8.0999 - val_mse: 8.0999\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.1422 - mse: 10.1422 - val_loss: 7.9993 - val_mse: 7.9993\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.0984 - mse: 10.0984 - val_loss: 7.8767 - val_mse: 7.8767\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.0526 - mse: 10.0526 - val_loss: 7.8063 - val_mse: 7.8063\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.0400 - mse: 10.0400 - val_loss: 7.8581 - val_mse: 7.8581\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.0401 - mse: 10.0401 - val_loss: 7.8801 - val_mse: 7.8801\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.9342 - mse: 9.9342 - val_loss: 7.7726 - val_mse: 7.7726\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.9346 - mse: 9.9346 - val_loss: 7.7952 - val_mse: 7.7952\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.8625 - mse: 9.8625 - val_loss: 7.9378 - val_mse: 7.9378\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.8722 - mse: 9.8722 - val_loss: 7.7058 - val_mse: 7.7058\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.7941 - mse: 9.7941 - val_loss: 7.6930 - val_mse: 7.6930\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.7319 - mse: 9.7319 - val_loss: 7.5504 - val_mse: 7.5504\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.6932 - mse: 9.6932 - val_loss: 7.6541 - val_mse: 7.6541\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.6975 - mse: 9.6975 - val_loss: 7.7150 - val_mse: 7.7150\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.6324 - mse: 9.6324 - val_loss: 7.7413 - val_mse: 7.7413\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.5767 - mse: 9.5767 - val_loss: 7.5983 - val_mse: 7.5983\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.6188 - mse: 9.6188 - val_loss: 7.5317 - val_mse: 7.5317\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.4457 - mse: 9.4457 - val_loss: 7.4527 - val_mse: 7.4527\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4823 - mse: 9.4823 - val_loss: 7.5037 - val_mse: 7.5037\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4631 - mse: 9.4631 - val_loss: 7.6477 - val_mse: 7.6477\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4016 - mse: 9.4016 - val_loss: 7.4615 - val_mse: 7.4615\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.3857 - mse: 9.3857 - val_loss: 7.4974 - val_mse: 7.4974\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.3667 - mse: 9.3667 - val_loss: 7.4961 - val_mse: 7.4961\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.2987 - mse: 9.2987 - val_loss: 7.2538 - val_mse: 7.2538\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.3581 - mse: 9.3581 - val_loss: 7.5359 - val_mse: 7.5359\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.3207 - mse: 9.3207 - val_loss: 7.5753 - val_mse: 7.5753\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.3236 - mse: 9.3236 - val_loss: 7.7153 - val_mse: 7.7153\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4344 - mse: 9.4344 - val_loss: 7.3814 - val_mse: 7.3814\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4256 - mse: 9.4256 - val_loss: 7.3934 - val_mse: 7.3934\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4243 - mse: 9.4243 - val_loss: 7.2729 - val_mse: 7.2729\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.0674 - mse: 9.0674 - val_loss: 7.4365 - val_mse: 7.4365\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.0706 - mse: 9.0706 - val_loss: 7.3010 - val_mse: 7.3010\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.0114 - mse: 9.0114 - val_loss: 7.2809 - val_mse: 7.2809\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.0685 - mse: 9.0685 - val_loss: 7.2562 - val_mse: 7.2562\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9780 - mse: 8.9780 - val_loss: 7.0248 - val_mse: 7.0248\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.9825 - mse: 8.9825 - val_loss: 7.0529 - val_mse: 7.0529\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8624 - mse: 8.8624 - val_loss: 7.2138 - val_mse: 7.2138\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7724 - mse: 8.7724 - val_loss: 7.1540 - val_mse: 7.1540\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7916 - mse: 8.7916 - val_loss: 7.0754 - val_mse: 7.0754\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7205 - mse: 8.7205 - val_loss: 7.0441 - val_mse: 7.0441\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8742 - mse: 8.8742 - val_loss: 7.0499 - val_mse: 7.0499\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6799 - mse: 8.6799 - val_loss: 7.0198 - val_mse: 7.0198\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7793 - mse: 8.7793 - val_loss: 6.9449 - val_mse: 6.9449\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.6818 - mse: 8.6818 - val_loss: 6.9856 - val_mse: 6.9856\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.5797 - mse: 8.5797 - val_loss: 6.9261 - val_mse: 6.9261\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.5321 - mse: 8.5321 - val_loss: 6.6978 - val_mse: 6.6978\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.5616 - mse: 8.5616 - val_loss: 6.7622 - val_mse: 6.7622\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.5225 - mse: 8.5225 - val_loss: 6.8376 - val_mse: 6.8376\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5465 - mse: 8.5465 - val_loss: 6.7545 - val_mse: 6.7545\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4330 - mse: 8.4330 - val_loss: 6.8732 - val_mse: 6.8732\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.5386 - mse: 8.5386 - val_loss: 6.8268 - val_mse: 6.8268\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2636 - mse: 8.2636 - val_loss: 6.7504 - val_mse: 6.7504\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4265 - mse: 8.4265 - val_loss: 6.6652 - val_mse: 6.6652\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.3178 - mse: 8.3178 - val_loss: 6.6442 - val_mse: 6.6442\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3116 - mse: 8.3116 - val_loss: 6.6475 - val_mse: 6.6475\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2527 - mse: 8.2527 - val_loss: 6.5885 - val_mse: 6.5885\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2459 - mse: 8.2459 - val_loss: 6.7506 - val_mse: 6.7506\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.1292 - mse: 8.1292 - val_loss: 6.6825 - val_mse: 6.6825\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.1105 - mse: 8.1105 - val_loss: 6.6539 - val_mse: 6.6539\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0942 - mse: 8.0942 - val_loss: 6.7656 - val_mse: 6.7656\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2252 - mse: 8.2252 - val_loss: 6.6660 - val_mse: 6.6660\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.1512 - mse: 8.1512 - val_loss: 6.4780 - val_mse: 6.4780\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.1098 - mse: 8.1098 - val_loss: 6.5901 - val_mse: 6.5901\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.9341 - mse: 7.9341 - val_loss: 6.6213 - val_mse: 6.6213\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0261 - mse: 8.0261 - val_loss: 6.4429 - val_mse: 6.4429\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9751 - mse: 7.9751 - val_loss: 6.4507 - val_mse: 6.4507\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8948 - mse: 7.8948 - val_loss: 6.2424 - val_mse: 6.2424\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8259 - mse: 7.8259 - val_loss: 6.3418 - val_mse: 6.3418\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8489 - mse: 7.8489 - val_loss: 6.4409 - val_mse: 6.4409\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8606 - mse: 7.8606 - val_loss: 6.4544 - val_mse: 6.4544\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7287 - mse: 7.7287 - val_loss: 6.3259 - val_mse: 6.3259\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8558 - mse: 7.8558 - val_loss: 6.3964 - val_mse: 6.3964\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7639 - mse: 7.7639 - val_loss: 6.3281 - val_mse: 6.3281\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6718 - mse: 7.6718 - val_loss: 6.4333 - val_mse: 6.4333\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6306 - mse: 7.6306 - val_loss: 6.2305 - val_mse: 6.2305\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5581 - mse: 7.5581 - val_loss: 6.2965 - val_mse: 6.2965\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5320 - mse: 7.5320 - val_loss: 6.1931 - val_mse: 6.1931\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.5195 - mse: 7.5195 - val_loss: 6.2775 - val_mse: 6.2775\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.5376 - mse: 7.5376 - val_loss: 6.0549 - val_mse: 6.0549\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4426 - mse: 7.4426 - val_loss: 6.2451 - val_mse: 6.2451\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4650 - mse: 7.4650 - val_loss: 6.4207 - val_mse: 6.4207\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4607 - mse: 7.4607 - val_loss: 6.3530 - val_mse: 6.3530\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4796 - mse: 7.4796 - val_loss: 6.2012 - val_mse: 6.2012\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.3257 - mse: 7.3257 - val_loss: 6.3869 - val_mse: 6.3869\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3355 - mse: 7.3355 - val_loss: 6.1867 - val_mse: 6.1867\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.4067 - mse: 7.4067 - val_loss: 6.1129 - val_mse: 6.1129\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3036 - mse: 7.3036 - val_loss: 6.3399 - val_mse: 6.3399\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2543 - mse: 7.2543 - val_loss: 6.3326 - val_mse: 6.3326\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2313 - mse: 7.2313 - val_loss: 6.0474 - val_mse: 6.0474\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.2207 - mse: 7.2207 - val_loss: 6.0809 - val_mse: 6.0809\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.1176 - mse: 7.1176 - val_loss: 6.1392 - val_mse: 6.1392\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0894 - mse: 7.0894 - val_loss: 6.0936 - val_mse: 6.0936\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.1766 - mse: 7.1766 - val_loss: 6.0179 - val_mse: 6.0179\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 7.0330 - mse: 7.0330 - val_loss: 5.9446 - val_mse: 5.9446\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.0530 - mse: 7.0530 - val_loss: 6.0576 - val_mse: 6.0576\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.0929 - mse: 7.0929 - val_loss: 5.8876 - val_mse: 5.8876\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.0379 - mse: 7.0379 - val_loss: 6.0164 - val_mse: 6.0164\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.0572 - mse: 7.0572 - val_loss: 5.8960 - val_mse: 5.8960\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.9690 - mse: 6.9690 - val_loss: 6.0109 - val_mse: 6.0109\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.9056 - mse: 6.9056 - val_loss: 6.1598 - val_mse: 6.1598\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.8756 - mse: 6.8756 - val_loss: 5.8874 - val_mse: 5.8874\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.8473 - mse: 6.8473 - val_loss: 5.9654 - val_mse: 5.9654\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.8206 - mse: 6.8206 - val_loss: 5.8653 - val_mse: 5.8653\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.8353 - mse: 6.8353 - val_loss: 5.9447 - val_mse: 5.9447\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.7732 - mse: 6.7732 - val_loss: 5.8054 - val_mse: 5.8054\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.9037 - mse: 6.9037 - val_loss: 5.7959 - val_mse: 5.7959\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.7616 - mse: 6.7616 - val_loss: 5.9258 - val_mse: 5.9258\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.6972 - mse: 6.6972 - val_loss: 5.7931 - val_mse: 5.7931\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.6982 - mse: 6.6982 - val_loss: 5.9469 - val_mse: 5.9469\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.7060 - mse: 6.7060 - val_loss: 5.7591 - val_mse: 5.7591\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.6140 - mse: 6.6140 - val_loss: 6.0789 - val_mse: 6.0789\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.6314 - mse: 6.6314 - val_loss: 6.1350 - val_mse: 6.1350\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.5775 - mse: 6.5775 - val_loss: 5.7415 - val_mse: 5.7415\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.5886 - mse: 6.5886 - val_loss: 5.6624 - val_mse: 5.6624\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5195 - mse: 6.5195 - val_loss: 5.7986 - val_mse: 5.7986\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5049 - mse: 6.5049 - val_loss: 5.8960 - val_mse: 5.8960\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5257 - mse: 6.5257 - val_loss: 5.8385 - val_mse: 5.8385\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.4970 - mse: 6.4970 - val_loss: 5.8660 - val_mse: 5.8660\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.5180 - mse: 6.5180 - val_loss: 5.7317 - val_mse: 5.7317\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4415 - mse: 6.4415 - val_loss: 5.6302 - val_mse: 5.6302\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.4169 - mse: 6.4169 - val_loss: 5.7715 - val_mse: 5.7715\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4250 - mse: 6.4250 - val_loss: 5.7183 - val_mse: 5.7183\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.3593 - mse: 6.3593 - val_loss: 5.7050 - val_mse: 5.7050\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3758 - mse: 6.3758 - val_loss: 5.8543 - val_mse: 5.8543\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3133 - mse: 6.3133 - val_loss: 5.7997 - val_mse: 5.7997\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.3353 - mse: 6.3353 - val_loss: 5.7276 - val_mse: 5.7276\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3409 - mse: 6.3409 - val_loss: 5.6168 - val_mse: 5.6168\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.2542 - mse: 6.2542 - val_loss: 5.7675 - val_mse: 5.7675\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.2450 - mse: 6.2450 - val_loss: 5.8562 - val_mse: 5.8562\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.2377 - mse: 6.2377 - val_loss: 5.8217 - val_mse: 5.8217\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1595 - mse: 6.1595 - val_loss: 5.7842 - val_mse: 5.7842\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3109 - mse: 6.3109 - val_loss: 5.6383 - val_mse: 5.6383\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.2220 - mse: 6.2220 - val_loss: 5.7153 - val_mse: 5.7153\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2231 - mse: 6.2231 - val_loss: 5.6947 - val_mse: 5.6947\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.0934 - mse: 6.0934 - val_loss: 5.8966 - val_mse: 5.8966\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1092 - mse: 6.1092 - val_loss: 5.7415 - val_mse: 5.7415\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.0992 - mse: 6.0992 - val_loss: 5.5437 - val_mse: 5.5437\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.1401 - mse: 6.1401 - val_loss: 5.5658 - val_mse: 5.5658\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1451 - mse: 6.1451 - val_loss: 5.7769 - val_mse: 5.7769\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0395 - mse: 6.0395 - val_loss: 5.7047 - val_mse: 5.7047\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0203 - mse: 6.0203 - val_loss: 5.7092 - val_mse: 5.7092\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.9954 - mse: 5.9954 - val_loss: 5.7318 - val_mse: 5.7318\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0128 - mse: 6.0128 - val_loss: 5.4424 - val_mse: 5.4424\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.0313 - mse: 6.0313 - val_loss: 5.5432 - val_mse: 5.5432\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.0405 - mse: 6.0405 - val_loss: 5.5775 - val_mse: 5.5775\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.0568 - mse: 6.0568 - val_loss: 5.4069 - val_mse: 5.4069\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.9131 - mse: 5.9131 - val_loss: 5.4496 - val_mse: 5.4496\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.8737 - mse: 5.8737 - val_loss: 5.4496 - val_mse: 5.4496\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9019 - mse: 5.9019 - val_loss: 5.6604 - val_mse: 5.6604\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.8891 - mse: 5.8891 - val_loss: 5.3230 - val_mse: 5.3230\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.8537 - mse: 5.8537 - val_loss: 5.3761 - val_mse: 5.3761\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8063 - mse: 5.8063 - val_loss: 5.4694 - val_mse: 5.4694\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.8242 - mse: 5.8242 - val_loss: 5.3631 - val_mse: 5.3631\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7775 - mse: 5.7775 - val_loss: 5.4216 - val_mse: 5.4216\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.7799 - mse: 5.7799 - val_loss: 5.3718 - val_mse: 5.3718\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51c05e72b0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "outputId": "7ca3e05b-2548-4a58-ec46-c0ea9cdd0ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "mse = np.square(errors)/n\n",
        "\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA03UlEQVR4nO3de3xT9f3H8dcnl6ZpS0HkolMUL4iKIBuoiCIVb9wENnWbigqbMn/eN69T53RuOm+bOnXOqSjiZfMKIjpULDrvIBdBdCKioigFhdI2TZPm8/vjnGIpvaSlyWmSz/PxyKPJyenJ+5uk+fSc8833K6qKMcaY3OXzOoAxxhhvWSEwxpgcZ4XAGGNynBUCY4zJcVYIjDEmxwW8DtBa3bp10969e3sdo9UqKyspLCz0OkZaWZuzX661FzK3zQsWLFinqt0buy/jCkHv3r2ZP3++1zFarbS0lJKSEq9jpJW1OfvlWnshc9ssIp81dZ8dGjLGmBxnhcAYY3KcFQJjjMlxVgiMMSbHWSEwxpgcl3G9htoiOm8ukelTSawrw9etO+GJkwkNH+F1LGOMSUqqP8OyvhBE582l8q5bIRoFIFG21rkNVgyMMR1eOj7Dsv7QUGT61M1P4GbRqLPcGGM6uHR8hmV9IUisK2vVcmOM6UjS8RmW9YXA163Rb1Q3udwYYzqSdHyGZX0hCE+cDKHQlguDQWe5McZ0cPknnLT1wlCoXT/Dsv5kcd3JlMj0qSTKygDFv2dfO1FsjMkMNc75AemyHbpxg/UaaqvQ8BGbn7TKe+4gOucFEhs24OvSxdtgxhjTDE0kqH5uBoG++1B8w60pe5ysPzTUUP7ocRCPEX1xttdRjDGmWbGFC0is+YrQmPEpfZycKwT+nXchMHAQ1c/PQuNxr+MYY0yTos89g2zXlbyDD03p4+RcIQDIHzMO/XY9NW+97nUUY4xpVO2Xq4m9N5/8kWOQYDClj5WThSA46EB8O+xI9LkZXkcxxphGVc+eCYEAoWPGpPyxcrIQiM9H/uhxxJcvI/7Jx17HMcaYLWhVJdG5L5J3yHB8XbZL+ePlZCEAyBtxNOTnU217BcaYDiY69yWIVJE/NrUnievkbCHwFRURKjmSmtdKSWzc4HUcY4wB3C6js2fi32tvAn36puUxc7YQgHPSmFiM6JznvY5ijDEAxBYtIPHVavJT3GW0vpwuBP5euxLY/4dEX7CupMaYjiH63Eyny+jQYWl7zJwuBAD5YyaQWL+O2NtveB3FGJPjar/6ktiCd8g/JvVdRuvL+UIQHHQAvp472EljY4znvu8yOjqtj5vzhUD8fkKjxxH/YCnxlZ94HccYk6M0UkXN3DnkHXIYvu26pvWxc74QAISOOAZCIdsrMMZ4JvrKS2hVVVpPEtexQkBdV9IjqHl1LonyjV7HMcbkmM1dRvvsRWCvvdP++FYIXKEx452upC++4HUUY0yOiS9eSGL1F+SPmeDJ41shcAV26U2g/0Cizz+L1tZ6HccYk0Oqn5uBdO5C3iHp6zJaX8oKgYj0EpFXROQDEVkmIuc3so6IyO0iskJElojIj1KVJxn5Y8eTWFdmXUmNMWlTu+YrYgveITRyDBLM8yRDKvcI4sCFqrovMAQ4W0T2bbDOKKCPe5kC/D2FeVoUHHwQvh497aSxMSZtqp9/Fnw+8tMwymhTUlYIVHWNqr7nXt8ELAd2arDaeGCaOt4CuojIjqnK1BKnK+mxxJe9T/xT60pqjEktjUSoeekF8oYOw9d1e89ypGXOYhHpDfwQeLvBXTsBX9S7vdpdtqbB70/B2WOgZ8+elJaWpioq/vwi9gkEWHXvP1h9VPt9qaOioiKluTsia3P2y7X2Qvu2uevi99i5qooPduxFlYfPY8oLgYgUAU8CF6hqeVu2oar3APcADB48WEtKStovYCMqV/4PX+lL7H7plfiKi9tlm6WlpaQ6d0djbc5+udZeaL82qyobn3gY2XMvDjjxZERk28O1UUp7DYlIEKcIPKyqTzWyypdAr3q3d3aXeSo0ehzU1BB9yUYlNcakRnzJQhKrPyd/zDhPiwCktteQAPcBy1X1L02sNhM41e09NATYqKprmlg3bQK9dyPQf3+iz8+yrqTGmJSonuV2GT10uNdRUrpHcAhwCjBCRBa5l9EicqaInOmuMxtYCawA/gmclcI8rZI/ZjyJsrXE3nnT6yjGmCxT+/UaYvPfJnT0aM+6jNaXsnMEqvpfoNn9HVVV4OxUZdgWwQOG4OvudCXNO/hQr+MYY7JIdPZMp8voSO+6jNZn3yxugvj9hEaNJb50CfFVK72OY4zJEhqJEH3pP+QdfCi+7bt5HQewQtCs0FEjIS9E9LmZXkcxxmSJ6LyX0apKQmmamD4ZVgia4etUTGj44UTnzSWxqU09X40xZjNVJfrcDPy770mgb8OBFrxjhaAFoTHjoSZK9KX/eB3FGJPh4ksWUfvF5+SPneB5l9H6rBC0INB7dwL9BhCdbaOSGmO2TfVzM5Dizh2iy2h9VgiSkD92PImyb4i923CEDGOMSU7tN18Te/ctQseMRvK87zJanxWCJAQPPBhft+5UP/eM11GMMRkq+vyzIOLpKKNNsUKQhM2jkr6/mPhnq7yOY4zJMFpdTfTFF5wuo926ex1nK1YIkhQ6chTk5RG1uQqMMa0UnTcXraxwOp90QFYIkuQrLiZ02AiipS+TqNjkdRxjTIZwuow+43QZ3aef13EaZYWgFUJjxrldSW2Ce2NMcuJLF1P7+WcdYpTRplghaIXAbnsQ6NffupIaY5JW/dxMp8vosMO9jtIkKwStlD9mPIm13xCbb11JjTHNq137DbF33iR01MgO12W0PisErRQ8aCi+7bvZBPfGmBZFn38WgNDIsR4naZ4VglZyRiU9lviSRcQ/X+V1HGNMB6XRaqIvPk9wyCH4u/fwOk6zrBC0QejoURAMOmOKG2NMI6KvvoJWVJDfQbuM1meFoA18xZ3JO+xwoq+8ZF1JjTFbUVWis2bg7707gX338zpOi6wQtFH+mPEQjRJ9eY7XUYwxHUx82fvUfvZphxtltClWCNoo4H45JDp7pnUlNcZsoXrWM0inYvKGlXgdJSktFgIRuVFEikUkKCIvi0iZiExMR7iOLn/sBBLffE1swbteRzHGdBC1ZWudLqNHj0JCIa/jJCWZPYKjVbUcGAusAvYELk5lqEwRPGgosn03G5XUGLNZpnQZrS+ZQhB0f44BHlfVjSnMk1EkECB/5FjiixdS+8VnXscxxnhMo1Gny+hBQzt8l9H6kikEM0XkQ2AQ8LKIdAeqUxsrc9R1Ja22Ce6NyXk1r76CbtqUEV1G62u2EIiID3gWGAoMVtUYUAVkVitTyNe5C3nDSoiWvkSiosLrOMYYj6gq1c/NwN97NwL9+nsdp1WaLQSqmgDuVNVvVbXWXVapql+nJV2GyB8zHqqrqZlrXUmNyVXxD5ZSu2oloTHjM6LLaH3JHBp6WUSOk0xrWRoF9uhDYJ9+VD9nXUmNyVXVz81AOnUidFjHHWW0KckUgl8BjwM1IlIuIptEpDzFuTJOaMw4Et+sIfaedSU1JtfUlq0l9tbrhI4ciYTyvY7Tai0WAlXtpKo+VQ2qarF7uzgd4TJJ3pBDka7b20ljY3JQ9IVZAIRGHetxkrZJ6pvFIjJORG52L5nTOTaNNnclXbSA2tWfex3HGJMmGo0SnfM8wQMPxt+jp9dx2iSZbxb/GTgf+MC9nC8i16c6WCYKHT0aAkGqbVRSY3JGzX9L0U3lGddltL5k9ghGA0ep6v2qej8wEufLZaYBX5cu5A0bTnTuSyQqK72OY4xJMVWletYM/Lv0JrDfAK/jtFmyg851qXe9cwpyZI38MROgOmJdSY3JAfHly6j99BNCYzOvy2h9yRSC64CFIvKAiDwILAD+lNpYmSuwZx8CffehevZMNJHwOo4xJoWis55BiooIDR/hdZRtksw3ixPAEOAp4EngYFX9VxqyZazQ2Akk1nxF7L35XkcxxqRIYl0ZNW+9TujIURnZZbS+ZL5ZfImqrlHVme4lqW8Vi8j9IrJWRJY2cX+JiGwUkUXu5ao25O+Q8g4+FNmuK9HZNsG9MdmqenOX0czvSJnMoaGXROQiEeklIl3rLkn83gM4J5ab85qqDnQvf0himxnB6Uo6hth786n98guv4xhj2pnW1DhdRgcfhL/nDl7H2WbJFIKfAWcDr+KcH1gAtHjMQ1VfBb7dpnQZLHTMGAgEqJ79rNdRjDHtrOa1UrR8I/ljJ3gdpV0EmrvTPUdwWQrPCRwsIouBr4CLVHVZEzmmAFMAevbsSWlpaYritK9ee/aleM7zLNhlDypisYzJ3V4qKiqszVku19oLULFpE+tmPo5s340l67+DLGi/qGrzK4jMV9XBbdq4SG9glqru18h9xUBCVStEZDRwm6r2aWmbgwcP1vnzM+MkbPzjjyi/+DykqIhERQX+7j0IT5yc8T0MklVaWkpJSYnXMdIq19qcS+2NzptLZPpUasvWIkDwiGPodO5vvI6VNBFZ0NRneSrPETRLVctVtcK9PhsIiki3bd1uR1L71ZcgglZUIECibC2Vd91KdN5cr6MZY1ohOm8ulXfdSsItAgCx10qz5m85ZecIWiIiO9QNbS0iB7pZ1m/rdjuSyPSp0HCPKxp1lhtjMkZk+lSIRrdcWJM9f8vNniMAUNXd2rJhEXkUKAG6ichq4Pe48x+r6t3A8cD/iUgciAA/15aOU2WYxLqyVi03xnRM2f633GQhEJFLVPVG9/oJqvp4vfuuU9XLm9uwqp7Ywv13AHe0Mm9G8XXrTqJsbaPLjTGZI9v/lps7NPTzetd/2+C+lr4fYIDwxMkQCm25MC/kLDfGZIzwxMnga/BxGcqev+XmDg1JE9cbu20aUdc7aIueBoccljO9hozJFv4ddoREAgoK0arKrOsB2Fwh0CauN3bbNCE0fASh4SMofeUVfvTSbOLvvYNGqpBwgdfRjDFJUFWqHvgnsl1Xutx1P/Pefjvrusw2d2ho/7o5ioEB7vW62/3TlC97iFAw6XR040YiTz3e8vrGmA4h9vabxJcvI/zzU5Bw2Os4KdFkIVBVf705igPu9brbwXSGzBaBPn3JG1ZC9YwnSaxf53UcY0wLNB6natp9+HbehdCRx3gdJ2WSnZjGtJPwxMmQSFD1yDSvoxhjWhCd8zyJr1ZTcOovEb/f6zgpY4Ugzfw9dyB/9LHUvPIi8VWfeh3HGNMEjVQR+ddDBPoNIHjAQV7HSSkrBB7IP+FEJFxAZNp9XkcxxjQh8tTj6MaNFEw6PaOnoUyGFQIP+DoVk3/Cz4m99y6xxQu9jmOMaSCxfh3VM54kb1gJgT59vY6Tck0WAhHZVK+n0FaXdIbMRvmjx+Pr3pOqB++1uY2N6WCqHpkGiUTWfGGsJc31GuqkqsXAbcBlwE7AzsClwK1pSZfFJC+P8MRJ1K5cQU2WjGBoTDaIr/qUmrlzyB99bFbMPpaMZA4NjVPVu1R1kzt09N+B8akOlgvyhpXg36MPkYcfQGtqvI5jjAEi0+5DCgrJP6HZ4dKySjKFoFJEThYRv4j4RORkoDLVwXKB+HwUTDqdxLoyqmc943UcY3JebPFCYu+9S/4JJ+LrVOx1nLRJphCcBPwU+Ma9nOAuM+0g2H8gwcEHUv3EYyTK7dSLMV7RRIKqB+/F170n+aPHeR0nrVosBKq6SlXHq2o3Ve2uqhNUdVUasuWMgtNOR6sjRP79sNdRjMlZNfPmUrtyBeGJk5C8PK/jpFWLhUBE9hKRl0VkqXt7gIhcmfpoucPfa1dCRxxD9IVZ1K75yus4xuQcrakh8vAD+PfoQ96wEq/jpF0yh4b+iTMfQQxAVZew5VwFph2ETzwF/P6smfrOmExSPesZEuvKnC+PNZx3IAck0+ICVX2nwbJ4KsLkMl/X7cmfcDw1r79K/H8feh3HmJyRKC+n+onHCA4+kGD/gV7H8UQyhWCdiOyBOweBiBwPrElpqhwVnnAC0mU7qqbeQ5ZN32xMhxX598NodYSC0073OopnkikEZwP/APYWkS+BC4AzUxkqV0k4TPjEU4gvX0bs7Te9jmNM1qtd8xXRF2YROuIY/L129TqOZ5otBCLiB85S1SOB7sDeqnqoqn6WlnQ5KHTkSHw796Jq2n1o3I7AGZNKkelTwe93ztHlsGYLgarWAoe61ytVdVNaUuUw8fspOPWXJL5aTfTF572OY0zWiv/vQ2pef5X8Ccfj67q913E81dycxXUWishM4HHqfaNYVZ9KWaocFzxgCIF+/Yk8Np1QyRE2v7Ex7UxVqZp6D9JlO8ITTvA6jueSOUeQD6wHRgDHupexqQyV60TE+ZLZxg02v7ExKbB5HuITs3ce4tZocY9AVXNjHNYOJrDX3uQdOpzqGU+SP2pszu+6GtNetpyHeKTXcTqEZL5ZnC8iZ4vIXSJyf90lHeFynTO/cS0Rm9/YmHYTfTE35iFujWQODT0E7AAcA8zDmZPAThqngX+HHQmNHkd07hzin63yOo4xGU8jVUQem06gX/+sn4e4NZIpBHuq6u+ASlV9EBgD2DOYJuG6+Y0fvNfrKMZkPGce4g0UTDoj6+chbo1kCkHM/blBRPYDOgM9UhfJ1OfrVEz+8Ta/sTHbKtfmIW6NZArBPSKyHfA7YCbwAXBjSlOZLeSPsfmNjdlWkUcfgkRtzsxD3BrJzEdwr6p+p6rzVHV3Ve2hqnenI5xxbDG/8auveB3HmIwT/2wV0blzCI0elzPzELdGi91HReSqxpar6h/aP45pSt6wEqpnPEnk4QfIGzos5ybOMGZbRB68FwkXEM6heYhbI6k5i+tdaoFRQO8UZjKNcOY3PoNE2Vqb39iYVvh+HuKf59Q8xK2RzBfKbql/W0RuBv6TskSmScEBAwkOcuY3Dh05El+xvamNac6W8xCP9zpOh9WWqXgKcL5LYDwQPu2XzvzGjz/idRRjOryaV1/J2XmIWyOZbxa/LyJL3Msy4CPg1iR+734RWVs313Ej94uI3C4iK9xt/6jV6XNQYJfehI44mujzz9r8xsY0Y/M8xLvvmZPzELdGMnsEY/l+sLmjgR+o6h1J/N4DQHMDeYwC+riXKcDfk9imAcInnmrzGxvTgupZz5AoW0vB5DNych7i1kjm2dlU7xIBikWka92lqV9S1VeBb5vZ7nhgmjreArqIyI6tyJ6zfF23J3/8cTa/sTFNsHmIWyeZ+QjeA3oB3wECdAE+d+9TYPc2PvZOwBf1bq92l201H7KITMHZa6Bnz56Ulpa28SG9U1FR0a65fd12oG9BAV/ddjMrjz8ZOuDX5du7zZkg19rcUdu747yX6BapYuk+A4i2c76O2uZtkUwheBF4WlVnA4jIKGCCqv4qpcnqUdV7gHsABg8erCUlJel66HZTWlpKe+euTsSouvtvDC0IkXfQ0HbddntIRZs7ulxrc0dsb+2ar9h4x02EjjyGg49r/0lnOmKbt1Uyh4aG1BUBAFV9HmiPT50vcfY06uzsLjNJCh05Et9OO9v8xsbU8/08xKd6HSVjJFMIvhKRK0Wkt3u5AmiP7iozgVPd3kNDgI2qutVhIdM0CQSc+Y2/tPmNjQGbh7itkikEJwLdgafdSw93WbNE5FHgTaCviKwWkV+KyJkicqa7ymxgJbAC+CdwVhvy57zggQcT2Hc/Io9NRyNVXscxxjOqStUD/7R5iNsgmW8WfwucD+COQrpBVTWJ32u2WLjbODvJnKYJIkLBpDMov+R8Ik8/TsFJp3kdyRhPxN55k/gHSyn4v/NsHuJWanKPQESuEpG93eshEZmL89/7NyJyZLoCmpYF9tqbvEMOo3rGkyS+Xe91HGPS7vt5iHvZPMRt0NyhoZ/hfIsY4DR33R7AcOC6FOcyrRQ+5RdQa/Mbm9wUffF5El/aPMRt1VwhqKl3COgY4FFVrVXV5STX7dSkkX+HHQmNOtbmNzY5Z8t5iId4HScjNVcIoiKyn4h0Bw4H5tS7ryC1sUxbhE84CckPE5l2n9dRjEmbyNPuPMSnnW7zELdRc4XgfOAJ4EPgr6r6KYCIjAZs8twOyFfszm+84B1iSxZ5HceYlEt8u96Zh/jQ4QT22tvrOBmryUKgqm+r6t6qur2qXltv+eyWegQZ7+SPnYCvew+qHvinzW9ssl7kkWlQa/MQbysbki/LSF4e4ZNtfmOT/baYh3gHG69yW1ghyEJ5hx2OdO9B5e038+2PR7LhjFOIzpvrdSxj2kV03lw2nHEK5ef/ClTx/cDmydpW1vsnC9W8Vopu+A7cQ0OJsrVU3nUrAKHhI7wLZsw2is6b67yXo1FngSqRqf/AFw7be3sbJLVHICJDReQkETm17pLqYKbtItOnQiy25cJo1CayMRkvMn3q90Wgjr23t1mLewQi8hCwB7AIqHUXK2DfXOqgEuvKWrXcmExh7+3USObQ0GBg32TGFzIdg69bdxJlaxtdbkwmk6IidNOmrZbbe3vbJHNoaCmwQ6qDmPYTnjgZQqGtlgcGHehBGmPaR3zlJ2hV1daz8YVC1n10GyVTCLoBH4jIf0RkZt0l1cFM24WGj6DwrAvwde8BIvi6dUd+sBM1r7xI/PNVXsczptUSFRVU3HCtM8T0lLO/f29370HhWRfYieJtlMyhoatTHcK0v9DwEVv8cSS+Xc/G35xNxQ3X0vnmvyFhGyXEZAZVpfL2m0msW0unP91McO99CY861utYWaXFPQJVndfYJR3hTPvxdd2eogt/S2LNV1Te8VfslI/JFNVPP07snTcpmHQGwb339TpOVmqxEIjIEBF5V0QqRKRGRGpFpDwd4Uz7Cvbfn/DESdS8/irR52Z4HceYFsXeX0xk+lTyDjmM0NgJXsfJWsmcI7gDZ2rKj4EwcDpwZypDmdTJ//FPCR44hKqp9xD78AOv4xjTpMS366m45Xp8P9iJwnN+bSOLplBSXyhT1RWA352PYCpgUwBlKBGh8LyL8XXrQcVNfyKxcYPXkYzZisbjVNx8HRqpouiSK+2cVoolUwiqRCQPWCQiN4rIr5P8PdNB+YqKKLr0SrR8IxW3XI/W1rb8S8akUWT6VOIfLKXwrAsI7NLb6zhZL5kP9FPc9c4BKoFewHGpDGVSL7D7nhROOYf4kkVEHpvudRxjNqt563Wqn3mC0KhjrVtomrTYfVRVPxORMLCjql6ThkwmTUJHjST24TKqH3+EQN99yBtsXzgz3qr96ksqb78Zf5++FPxiitdxckYyvYaOxRln6AX39kD7Qln2KJxyDv7eu1N56w3UfvO113FMDtNoNRU3Xgt+P0UXX4EE87yOlDOSOTR0NXAgsAFAVRcBu6UskUkrCYUouvRKSCSouOlPaKzG60gmB6kqlf+4g9rPVlH460vx9+jpdaSckkwhiKnqxgbL7NtIWcS/404UnncxtSv+R9V9d3sdx+Sg6EsvUDP3RfJ/ehJ5PzrA6zg5J5lCsExETgL8ItJHRP4GvJHiXCbN8oYMJX/CCURfeI5o6ctexzE5JP7Jx1TdcyeBgYMI//Rkr+PkpGQKwblAPyAKPAqUAxekMJPxSPiUyQT69afyrtuIf7bK6zgmByQqNlFxw7X4ijtT9JtLEb/f60g5KZmxhqpU9QpVPUBVB7vXq9MRzqSX+P0UXfhbpLCQihuuRasqvY5kspgmElTeehOJb9dTdMmV+Io7ex0pZzXZfbSlnkGqOq794xiv+bpuT9FFl7Ppd5dQccdfnd4b9tV+kwLVT/2b2Py3KTjjLAJ99/E6Tk5r7nsEBwNf4BwOehuwT4McEezXn/ApvyDy4L1En32a/HE/8TqSyTKx9xcReeRB8oaVEBpt/1N6rblDQzsAlwP7AbcBRwHrbBjq3JA/4XiCBw2l6sF7iS1f5nUck0US69dRcfP1+H6wM4VnXWB7nB1Ak4XAHWDuBVU9DRgCrABKReSctKUznhERCs+9EF93d3C6DRu8jmSywObB5KLVdLr0d0g47HUkQwsni0UkJCI/AaYDZwO3A0+nI5jxnjM43e/Qik02OJ1pF5Fp9xFfvozCs3+Nv9cuXscxriYLgYhMA94EfgRc4/YaulZVv0xbOuO5wG57UPirc4i/v4jIo9O8jmMyWM0br1E98ylCo8cRGlbidRxTT3N7BBOBPsD5wBsiUu5eNiU7Q5mIjBSRj0RkhYhc1sj9k0SkTEQWuZfT29YMk0qhI44hdORIqp94jJp33/I6jslAtV+upuJvf8G/194UTLbB5Dqa5s4R+FS1k3sprnfppKrFLW1YRPw4M5mNAvYFThSRxiYc/ZeqDnQv97a5JSalCs44C/9ue1B56002OJ1pFa2upuKGa5FAwB1MLuh1JNNAKieYORBYoaorVbUGeAwYn8LHMynkDE73O1B1vmxWY4PTmZapKpV3307tF59R9JvL8Hfv4XUk0whRTc34cSJyPDBSVU93b58CHKSq59RbZxJwPVAG/A/4tap+0ci2pgBTAHr27DnoscceS0nmVKqoqKCoqMjrGNus+JOP6f3sE6zfbyBfHjmq2XWzpc2tkWttbqm9XZcsZOe5L/D1kENZO2RYGpOlTqa+xocffvgCVR3c6J2qmpILcDxwb73bpwB3NFhneyDkXv8VMLel7Q4aNEgz0SuvvOJ1hHZTOe0+XT/+aK2eO6fZ9bKpzcnKtTY3197Yxx/p+uPGaPk1l2uitjZ9oVIsU19jYL428bmaykNDX+JMa1lnZ3dZ/SK0XlWj7s17gUEpzGPaSfik0wj035/Kv/+N+KpPvY5jOqDEpnIqbvgjvu22o/CCSxGfTXPekaXy1XkX6CMiu4lIHvBzYIvxi0Rkx3o3xwHLU5jHtJMtBqe78VoSlTY4nfne5sHkvltP0cVX4CtusW+J8VjKCoGqxnEmvP8Pzgf8v1V1mYj8QUTqBhc5T0SWichi4DxgUqrymPbl67IdRRddTuLrNVT+7Za6Q33GUP3kY8QWvEPBL84ksNfeXscxSWhx8vptoaqzgdkNll1V7/pvgd+mMoNJnWC//oRP/SWRB/5J9cynCI8/zutIxmOxxQuJPPoQeYcdTmjUWK/jmCTZgTuzTfLHH0dwyCFEHryX2LL3vY5jPJRYv46Kv1yPfycbTC7TWCEw22Tz4HQ9d6Di5utIfPet15GMBzQWo+LGP6LRGoou/R2Sn+91JNMKVgjMNvMVFjqD01VW2uB0OarqwXuJf7ScwnN+jX9nG0wu06T0HIHJHYHeu1N45rlU3n4zm/78BxKrVtK/bC0bHr6P8MTJhIaP8DqiaUfReXOJTJ9K/7K1fHf/nWh5OaGxEwgdOtzraKYNrBCYdhMacRTVc+cQdwemEyBRtpbKu2517rdikBWi8+Y6r2k0igBaXg4i+Hffw+topo3s0JBpV/r1mq0XRqNEpk9NfxiTEpHpUyEa3XKhKtWPPuRNILPNrBCYdpVYv67x5evK0pzEpEpTr6W9xpnLCoFpV75u3Rtfvn3jy03mkaJOjS5v6rU3HZ8VAtOuwhMnQyi09R2FhSTKk5rPyHRQGotR+c+70E3OOYEthELOa28ykhUC065Cw0dQeNYF+Lr3QAFf9x4EjziGxJerKb/wHOIrPvY6ommDxPp1bLryYqLPzSA07icUnHvhFq9x4VkXWGeADGa9hky7Cw0fQWj4CEpLSykpKQEgPnIMFTdcS/lvf03hlHMIHTXS25AmabH3F1Nx83VotJrCiy7f3EU0f8RRW7zGJnPZHoFJi0CfvhT/5U4C++5H5Z1/pfLOv9osZx2cqhJ55nE2/f4ypFMnOt90u31PIEvZHoFJG19xZzpd9Scijz1E9eOPEl/5CUWXXIm/5w5eRzMNaKSKittvIfbmfwkOHUbRub9BwgVexzIpYnsEJq3E76fg5EkUXX41iTVfUX7ROdQsnO91LFNP7RefsfGic4m9/QbhSWc4E85bEchqVgiMJ/IOPJjiW/6Gr2s3Kv5wJZF/PYwmEl7HynnR/85j48XnoRUVdPrDDYQnHG+jiOYAKwTGM/4dd6L4xlvJO2wEkUenUXHd70lUbPI6Vk7SeJzK++6m8ubrCPTenc5/uZPgfgO8jmXSxAqB8ZSE8im84GIKppxNbNF7lF94LvGVn3gdK6ckvl3Ppt9dQvTZpwmNnUCna2/Et303r2OZNLJCYDwnIuSPHkfxn25G4zHKL7uA6NwXvY6VE2IfLGXjhecQX7mCwt9cRuHp/4cEg17HMmlmhcB0GIG++9D5ljsJ9N2HyttvpvLu29GYdTFNBVWleuZTbPrdJUg4TPGNtxM67HCvYxmPWPdR06H4unSh09XXE3n4Aaqf+jfxT1Y4XUy79/A6WtbQSITKO/9KzX/nETxoKIXnXYSvsNDrWMZDtkdgOhzx+yk49ZcUXXYVtau/oPzCc4gtfs/rWFmhdvXnlF9yHjVvvEb41F9QdNlVVgSMFQLTceUNOYTON/8NX5cubLrmCiKPP2pdTLdBzRuvsfGi80iUb6TT1dcT/snPrGuoAawQmA7Ov9POFN9wG3mHHEbk4Qeo+PM1JCoqvI6VUbS2lqoH7qXixj/i32VXOt9yJ8EBA72OZToQKwSmw5NwmMLfXEbB6f9HbMG7lF98LvFVK72OlRESG75j0+8vo/qZxwmNOpbiP91k8waYrVghMBlBRMgfO4FOf7wJra6m/JILiJa+7HWsDi324Qds/M3ZxP/3EYXnX0zhr85BgnlexzIdkBUCk1GC+/Sj81/uJNBnLypvvZHKe+5AYzGvY3Uoqkr1czPYdOXFSF4exTfeSujwI72OZTow6z5qMo5vu650uubPRB66n+oZTxL/ZAWdLr7CDnkAWl1N5d9vo2beXIIHDKHw/IvxFRV5Hct0cFYITEaSQICCyVMI9N2HittvYeOFZ5N31Ehi814hsa4MX7fuhCdOzvpZs6Lz5hKZPpXEujJku66AoN+tJ3zyJPKP+xnis51+0zIrBCaj5Q0dRudeu7LxqkuJPvGvzcsTZWupvOtWgKwtBtF5c502RqMA6LfrAQj95ATCJ5zoYTKTaezfBZPx/L12wef3b31HNEpk+tT0B0qTqgfv3VwE6ou9Ns+DNCaT2R6ByQqJ9esaX162loq/3kBwwEAC/Qfi79EzzcnaT6J8I/GlS4gtWUhsyeLNewBbrbeuLM3JTKazQmCygq9bdxJla7e+IxQitnghNfPmOuvtsCPB/gMJ7D+Q4H4D8XXpkt6graCRKmLLlhJfspDY+4up/dQdnjtcQLBff3TjBrRy6y/X2Ulz01pWCExWCE+cvMXxcgBCIQrPuoC8ww6n9ovPiC9ZRGzJImpef5Xoi88D4N+lN4EBA509hn4DPB13R2tqiH+0nNiSRcTfX0T844+gthaCQQJ79yN88mkEB/wQ/x59kEBgq3MEAIRChCdO9qwNJjNZITBZoe6EcF0Pmoa9hgK79CawS2/yx05Aa2upXbmCmFsYonOeJzrrGfD58O/Rh2BdYdi7HxIKpSyz1tZS+8nHTo73FxFfvgxqapwcffqS/+MTCA74IYG++zSao6U2G5MsKwQma4SGj0jqQ1D8fgJ9+hLo05fwcT9DYzXEP/qQ2JKFxN9fTPUzT1D95L8gECSw9z7Oh/GA/Qns2RcJtP1PRlWp/fwz91DPIuJLl6BVVQD4d92N0DFjnAK0b/+k90ySbbMxzUlpIRCRkcBtgB+4V1X/3OD+EDANGASsB36mqqtSmcmYhiSYR3C/AZvn6NVIFbEPlm0+Nh95dBo8opAfJtivv3Moqf9A/L1329xPv64/f/+ytWx4+D7CEyeTd9jhJL75enOBiS1ZhG7cAIBvxx+Qd2iJs6399u/Q5ypM9ktZIRARP3AncBSwGnhXRGaq6gf1Vvsl8J2q7ikiPwduAH6WqkzGJEPCBeQNOoC8QQcAkCgvJ7508eZDOLEF7xABpFMxgf77QzhM7NVSiNUguN9huO0mKu/9O2wqd7a5XVeCAwe5vZf2z+jeSyb7pHKP4EBghaquBBCRx4DxQP1CMB642r3+BHCHiIiqagpzGdMqvuJi8oYOI2/oMMDpnhlz/8OPL1nYeNfVRAKiUQqmnENwwP74duplY/+bDiuVhWAn4It6t1cDBzW1jqrGRWQjsD2wxV+WiEwBpgD07NmT0tLSFEVOnYqKiozMvS2yus0SgP0Hw4BB9L/tzzT2Ea81Ud4q6AQrVjqXLJTVr3ETsrHNGXGyWFXvAe4BGDx4sJaUlHgbqA1KS0vJxNzbIlfavOGR+xv9DoO/e4+sb3+uvMb1ZWObUznExJdAr3q3d3aXNbqOiASAzjgnjY3JGOGJk6Fh907rz28ySCoLwbtAHxHZTUTygJ8DMxusMxM4zb1+PDDXzg+YTBMaPoLCsy7A170HCvi696DwrAusW6fJGCk7NOQe8z8H+A9O99H7VXWZiPwBmK+qM4H7gIdEZAXwLU6xMCbj1PXnz8bDBib7pfQcgarOBmY3WHZVvevVwAmpzGCMMaZ5Ngy1McbkOCsExhiT46wQGGNMjrNCYIwxOU4yrbemiJQBn3mdow260eAb0znA2pz9cq29kLlt3lVVG521KOMKQaYSkfmqOtjrHOlkbc5+udZeyM4226EhY4zJcVYIjDEmx1khSJ97vA7gAWtz9su19kIWttnOERhjTI6zPQJjjMlxVgiMMSbHWSHwgIhcKCIqIt28zpJKInKTiHwoIktE5GkR6eJ1plQRkZEi8pGIrBCRy7zOk2oi0ktEXhGRD0RkmYic73WmdBERv4gsFJFZXmdpL1YI0kxEegFHA597nSUNXgT2U9UBwP+A33qcJyVExA/cCYwC9gVOFJF9vU2VcnHgQlXdFxgCnJ0Dba5zPrDc6xDtyQpB+v0VuATI+rP0qjpHVePuzbdwZqnLRgcCK1R1parWAI8B4z3OlFKqukZV33Ovb8L5YNzJ21SpJyI7A2OAe73O0p6sEKSRiIwHvlTVxV5n8cAvgOe9DpEiOwFf1Lu9mhz4UKwjIr2BHwJvexwlHW7F+Ucu4XGOdpURk9dnEhF5CdihkbuuAC7HOSyUNZprr6rOcNe5AudQwsPpzGZST0SKgCeBC1S13Os8qSQiY4G1qrpAREo8jtOurBC0M1U9srHlItIf2A1YLCLgHCZ5T0QOVNWv0xixXTXV3joiMgkYCxyRxfNRfwn0qnd7Z3dZVhORIE4ReFhVn/I6TxocAowTkdFAPlAsItNVdaLHubaZfaHMIyKyChisqpk4imFSRGQk8BdguKqWeZ0nVUQkgHMy/AicAvAucJKqLvM0WAqJ89/Mg8C3qnqBx3HSzt0juEhVx3ocpV3YOQKTSncAnYAXRWSRiNztdaBUcE+InwP8B+ek6b+zuQi4DgFOAUa4r+0i9z9lk4Fsj8AYY3Kc7REYY0yOs0JgjDE5zgqBMcbkOCsExhiT46wQGGNMjrNCkCYiUut2sVsmIovdEUh97n2DReR293pIRF5y1/2ZiAxzf2eRiIS9bUXjRKSiletPyLQBykSkt4ictI3bKBWRdp/0vD22KyIlIjK03u0zReTUbU8HInJ5G35nkojc0R6P34bH3uK5yAVWCNInoqoDVbUfcBTOSJW/B1DV+ap6nrveD91lA1X1X8DJwPXu7UhLDyKOjv66TsAZpTOT9Aa2qRB0cCXA5g8/Vb1bVae107ZbXQg8VkK95yInqKpd0nABKhrc3h1YDwjOG28W0ANYAWwEFgG/Ar4FPsX5Gj/AxTjfXF0CXOMu6w18BEwDlgG7NrPecuCf7npzgLB7357AS8Bi4D1gj6Yer7G24Yyqugx4GejuLt8DeAFYALwG7I3zB1bXpkXAQcACd/39cUZl3cW9/QlQAHTHGcrgXfdyiHt/IXA/8A6wEBjvLp8EPOU+9sfAjU3kvsrd3lKceWilqecCZ/TUutfl1+5j3FFvW7OAEvf634H57vNxTb11SnG+TZ5sjlLgBrd9/wOGucvDOCOcLgeexhnsrbHtDgLmuc//f4Ad3eXnAR+4r+lj7vvia5xvRS8ChgFX43xzti7HX902LQcOcJ/fj4E/1nu8Z9zHWgZMcZf9Gah1t1v3Hp7otmkR8A/A7y6f7LbzHZz36B2NtKmr+zhL3NdkgLt8c1739lK3Xb2BD3HGuVoOPAEUuOusArq51we77WzsuTjB3d5i4FWvP0tS8vnkdYBcudCgELjLNgA9cQuBu2zzdff2A8Dx7vWjcT8ocPbmZgGHuW/eBDAkifXiwEB3vX8DE93rbwM/dq/n43wAN7qdRtqhwMnu9avq/oBxikIf9/pBwNyGbXJvLwOKcb6d+y7OXtCuwJvu/Y8Ah7rXdwGWu9evq5e/C86HSCHOh/RKoLPbls+AXo3k7lrv+kPAsc08Fw1fl0k0XQi6uj/9OB8udR9WpTT+gd1UjlLgFvf6aOAl9/pvgPvd6wPc13Rwg20GgTf4vij/rN7vfAWE6p439+fVbPlBuvm2m+MG9/r57u/vCIRwRlrdvkG7wzgfnHXLK+ptdx/gWSDo3r4LONXd3uc4RT8PeJ3GC8HfgN+710cAi5rIX78QKN//83B/vXatokEhaGJb7wM71X++su1ig85llqPdy0L3dhHQB+cP6DNVfSuJ9T5V1UXu8gVAbxHphPNGfxpAVasBRKSp7bzaIFcC+Jd7fTrwlDsq5VDgcXeQPXA+OBrzBs6QBYfhfLiPxCk+r7n3HwnsW287xe72j8YZBOwid3k+TqEAeFlVN7rt+ACnsNQfKhrgcBG5BOeDviuwTERKm3gumojeqJ+KyBScQR13xDkMtqSZ9bfKgfNhCc5/3uC+Vu71w4Db3XxLRKSxbfcF9sMZ3gOcorTGvW8J8LCIPIPz33UyZro/3weWqeoaABFZiTPg3nrgPBH5sbteL5z3yvoG2zkCZ0/lXTdXGFiL849CqbpjUonIv4C9GslxKHAcgKrOFZHtRaS4hexfqOrr7vXpOHtEN7fY4u+9DjwgIv/m+9cjq1gh8IiI7I6zy7wW57+kpH4N53zBPxpsqzdQmeR60XqLanH+EFv1eElQnD2IDao6MIn1X8XZBd8VmAFc6m7jOfd+H87eTvUW4ZxPkuNU9aMGyw9i63YGGqyTj/Pf6GBV/UJErsYpJMmKs+U5tnx3u7sBFwEHqOp3IvJAc9tNIkddO7ZqQwsE5wP74EbuG4NTTI4FrnBHxm1JXY4EWz63CSDgDsJ2JHCwqla5BbWxdgvwoKpuMVudiExIIkNzGn09XA3H0am7Xf93mnyNVPVM9z01BlggIoNUtWGBy2gd/aRiVhKR7sDdOLu+rRns6T/AL9z/hhGRnUSkxzasB2yeYWp13R+j23OpoBXb8QHHu9dPAv6rztj0n4rICe7viojs766zCWcwujqv4Rw3/lhVEzjnEEYD/3XvnwOcW7eyiAys185z3YKAiPywqTY2ou4Pf53bvuNbeC4aZl4FDBQRnzjTjx7oLi/GKcobRaQnTqeAVudowau4J65FZD+cw0MNfQR0F5GD3fWCItLP7UjQS1VfwSm4nXH29Bq2r7U6A9+5RWBvnOkr68TEGbIanMOFx9e9j0Skq4jsinM4brj7H34Q57h8Y17DOXRYNwLoOve9tgr4kbv8RzhDvtfZpe55wH1/utdX4eydgLuX4driuRCRPVT1bVW9CihjyyHHs4IVgvQJu11Al+GciJwDXNOaDajqHJzj5W+KyPs4J762+uNNdr0GTsHZtV+Cc6hmh1ZspxI4UESW4hy3/YO7/GTglyKyGOdwR930jY8BF4szAfgeqroK5z/FukNO/8XZm/jOvX0eMFhElriHec50l1+Lcyx8ifu8XttCGzdT1Q04JySX4hSUd5t7LnAOp9SK0/X31ziHCz7FOel6O85JZdSZfW4hzgnKR9z12pqjKX8HikRkOc5zvaCR7dbgFJUb3Od/Ec6hOj8w3X09FwK3uxmeBX7svkeHJZGhoRdw9gyW45wgfqvefffgvEYPq+oHwJXAHPf5fRHnJPYanGPzb+I8Z03NCXw1MMj93T8Dp7nLnwS6uu+Dc3DOF9X5CGdO5eXAdjjPHzh/f7eJyHycPa46DZ+Lm0Tkfff9/QbOSeOsYqOPGmOylns4dJaq7ud1lo7M9giMMSbH2R6BMcbkONsjMMaYHGeFwBhjcpwVAmOMyXFWCIwxJsdZITDGmBz3/5lVcMor950oAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "outputId": "305f8d13-a8c8-4266-89e6-c30e10ef6711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 2s 19ms/step - loss: 22.0970 - mae: 22.0970 - val_loss: 20.6626 - val_mae: 20.6626\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 21.0981 - mae: 21.0981 - val_loss: 19.2372 - val_mae: 19.2372\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 19.0560 - mae: 19.0560 - val_loss: 16.3469 - val_mae: 16.3469\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15.1003 - mae: 15.1003 - val_loss: 11.1795 - val_mae: 11.1795\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.7345 - mae: 9.7345 - val_loss: 7.2386 - val_mae: 7.2386\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5411 - mae: 7.5411 - val_loss: 6.0012 - val_mae: 6.0012\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8241 - mae: 5.8241 - val_loss: 4.3432 - val_mae: 4.3432\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.5386 - mae: 4.5386 - val_loss: 3.9578 - val_mae: 3.9578\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.8746 - mae: 3.8746 - val_loss: 3.5583 - val_mae: 3.5583\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3.6070 - mae: 3.6070 - val_loss: 3.4576 - val_mae: 3.4576\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3.4346 - mae: 3.4346 - val_loss: 3.1412 - val_mae: 3.1412\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.2198 - mae: 3.2198 - val_loss: 3.1041 - val_mae: 3.1041\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3.0603 - mae: 3.0603 - val_loss: 2.9630 - val_mae: 2.9630\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.9251 - mae: 2.9251 - val_loss: 2.9325 - val_mae: 2.9325\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.7965 - mae: 2.7965 - val_loss: 2.8468 - val_mae: 2.8468\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.6917 - mae: 2.6917 - val_loss: 2.8220 - val_mae: 2.8220\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.6062 - mae: 2.6062 - val_loss: 2.6966 - val_mae: 2.6966\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.5360 - mae: 2.5360 - val_loss: 2.6281 - val_mae: 2.6281\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.4762 - mae: 2.4762 - val_loss: 2.6280 - val_mae: 2.6280\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.4676 - mae: 2.4676 - val_loss: 2.5228 - val_mae: 2.5228\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.3872 - mae: 2.3872 - val_loss: 2.4181 - val_mae: 2.4181\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.3612 - mae: 2.3612 - val_loss: 2.7243 - val_mae: 2.7243\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.2736 - mae: 2.2736 - val_loss: 2.2901 - val_mae: 2.2901\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.2339 - mae: 2.2339 - val_loss: 2.6144 - val_mae: 2.6144\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.2473 - mae: 2.2473 - val_loss: 2.5142 - val_mae: 2.5142\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2111 - mae: 2.2111 - val_loss: 2.3918 - val_mae: 2.3918\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1691 - mae: 2.1691 - val_loss: 2.4309 - val_mae: 2.4309\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1164 - mae: 2.1164 - val_loss: 2.3656 - val_mae: 2.3656\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0999 - mae: 2.0999 - val_loss: 2.3806 - val_mae: 2.3806\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.0865 - mae: 2.0865 - val_loss: 2.3527 - val_mae: 2.3527\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0923 - mae: 2.0923 - val_loss: 2.3876 - val_mae: 2.3876\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0981 - mae: 2.0981 - val_loss: 2.3032 - val_mae: 2.3032\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.0823 - mae: 2.0823 - val_loss: 2.3813 - val_mae: 2.3813\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0487 - mae: 2.0487 - val_loss: 2.3663 - val_mae: 2.3663\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9818 - mae: 1.9818 - val_loss: 2.3093 - val_mae: 2.3093\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0184 - mae: 2.0184 - val_loss: 2.4020 - val_mae: 2.4020\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9663 - mae: 1.9663 - val_loss: 2.3143 - val_mae: 2.3143\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9858 - mae: 1.9858 - val_loss: 2.2900 - val_mae: 2.2900\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0316 - mae: 2.0316 - val_loss: 2.3416 - val_mae: 2.3416\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0399 - mae: 2.0399 - val_loss: 2.4165 - val_mae: 2.4165\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9371 - mae: 1.9371 - val_loss: 2.3316 - val_mae: 2.3316\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9385 - mae: 1.9385 - val_loss: 2.3046 - val_mae: 2.3046\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9230 - mae: 1.9230 - val_loss: 2.3346 - val_mae: 2.3346\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8792 - mae: 1.8792 - val_loss: 2.3486 - val_mae: 2.3486\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8853 - mae: 1.8853 - val_loss: 2.3115 - val_mae: 2.3115\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9107 - mae: 1.9107 - val_loss: 2.3133 - val_mae: 2.3133\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8872 - mae: 1.8872 - val_loss: 2.3816 - val_mae: 2.3816\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8684 - mae: 1.8684 - val_loss: 2.2316 - val_mae: 2.2316\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8508 - mae: 1.8508 - val_loss: 2.3073 - val_mae: 2.3073\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8230 - mae: 1.8230 - val_loss: 2.2865 - val_mae: 2.2865\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8170 - mae: 1.8170 - val_loss: 2.2417 - val_mae: 2.2417\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8133 - mae: 1.8133 - val_loss: 2.3013 - val_mae: 2.3013\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8304 - mae: 1.8304 - val_loss: 2.2976 - val_mae: 2.2976\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8465 - mae: 1.8465 - val_loss: 2.4271 - val_mae: 2.4271\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7977 - mae: 1.7977 - val_loss: 2.2453 - val_mae: 2.2453\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7820 - mae: 1.7820 - val_loss: 2.4975 - val_mae: 2.4975\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8402 - mae: 1.8402 - val_loss: 2.2852 - val_mae: 2.2852\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7978 - mae: 1.7978 - val_loss: 2.2679 - val_mae: 2.2679\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7486 - mae: 1.7486 - val_loss: 2.3129 - val_mae: 2.3129\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.7318 - mae: 1.7318 - val_loss: 2.2681 - val_mae: 2.2681\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.7425 - mae: 1.7425 - val_loss: 2.4721 - val_mae: 2.4721\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.8529 - mae: 1.8529 - val_loss: 2.2518 - val_mae: 2.2518\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.7189 - mae: 1.7189 - val_loss: 2.2747 - val_mae: 2.2747\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.7054 - mae: 1.7054 - val_loss: 2.2231 - val_mae: 2.2231\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6989 - mae: 1.6989 - val_loss: 2.3064 - val_mae: 2.3064\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7085 - mae: 1.7085 - val_loss: 2.1794 - val_mae: 2.1794\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.7187 - mae: 1.7187 - val_loss: 2.3117 - val_mae: 2.3117\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.7112 - mae: 1.7112 - val_loss: 2.2528 - val_mae: 2.2528\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6753 - mae: 1.6753 - val_loss: 2.2316 - val_mae: 2.2316\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6595 - mae: 1.6595 - val_loss: 2.1586 - val_mae: 2.1586\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6735 - mae: 1.6735 - val_loss: 2.2744 - val_mae: 2.2744\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6671 - mae: 1.6671 - val_loss: 2.2451 - val_mae: 2.2451\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.7230 - mae: 1.7230 - val_loss: 2.1787 - val_mae: 2.1787\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6438 - mae: 1.6438 - val_loss: 2.2272 - val_mae: 2.2272\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6471 - mae: 1.6471 - val_loss: 2.2868 - val_mae: 2.2868\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.6101 - mae: 1.6101 - val_loss: 2.3120 - val_mae: 2.3120\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.6982 - mae: 1.6982 - val_loss: 2.1463 - val_mae: 2.1463\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.7293 - mae: 1.7293 - val_loss: 2.3263 - val_mae: 2.3263\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.6533 - mae: 1.6533 - val_loss: 2.1888 - val_mae: 2.1888\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6666 - mae: 1.6666 - val_loss: 2.2797 - val_mae: 2.2797\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6032 - mae: 1.6032 - val_loss: 2.1854 - val_mae: 2.1854\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6051 - mae: 1.6051 - val_loss: 2.1594 - val_mae: 2.1594\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6030 - mae: 1.6030 - val_loss: 2.2665 - val_mae: 2.2665\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5919 - mae: 1.5919 - val_loss: 2.1912 - val_mae: 2.1912\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6005 - mae: 1.6005 - val_loss: 2.1235 - val_mae: 2.1235\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5907 - mae: 1.5907 - val_loss: 2.1568 - val_mae: 2.1568\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.5911 - mae: 1.5911 - val_loss: 2.2031 - val_mae: 2.2031\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5957 - mae: 1.5957 - val_loss: 2.1567 - val_mae: 2.1567\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5564 - mae: 1.5564 - val_loss: 2.2145 - val_mae: 2.2145\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.6164 - mae: 1.6164 - val_loss: 2.2488 - val_mae: 2.2488\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6032 - mae: 1.6032 - val_loss: 2.1869 - val_mae: 2.1869\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6800 - mae: 1.6800 - val_loss: 2.1972 - val_mae: 2.1972\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5738 - mae: 1.5738 - val_loss: 2.1726 - val_mae: 2.1726\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5500 - mae: 1.5500 - val_loss: 2.1889 - val_mae: 2.1889\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5775 - mae: 1.5775 - val_loss: 2.1637 - val_mae: 2.1637\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6579 - mae: 1.6579 - val_loss: 2.2334 - val_mae: 2.2334\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5653 - mae: 1.5653 - val_loss: 2.1673 - val_mae: 2.1673\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.5164 - mae: 1.5164 - val_loss: 2.1432 - val_mae: 2.1432\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4974 - mae: 1.4974 - val_loss: 2.1533 - val_mae: 2.1533\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5029 - mae: 1.5029 - val_loss: 2.1230 - val_mae: 2.1230\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4870 - mae: 1.4870 - val_loss: 2.1749 - val_mae: 2.1749\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5051 - mae: 1.5051 - val_loss: 2.1302 - val_mae: 2.1302\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4880 - mae: 1.4880 - val_loss: 2.1531 - val_mae: 2.1531\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5025 - mae: 1.5025 - val_loss: 2.1473 - val_mae: 2.1473\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4782 - mae: 1.4782 - val_loss: 2.1038 - val_mae: 2.1038\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5721 - mae: 1.5721 - val_loss: 2.1854 - val_mae: 2.1854\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5247 - mae: 1.5247 - val_loss: 2.1011 - val_mae: 2.1011\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5133 - mae: 1.5133 - val_loss: 2.1724 - val_mae: 2.1724\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5119 - mae: 1.5119 - val_loss: 2.1206 - val_mae: 2.1206\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4718 - mae: 1.4718 - val_loss: 2.2143 - val_mae: 2.2143\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4969 - mae: 1.4969 - val_loss: 2.0721 - val_mae: 2.0721\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5138 - mae: 1.5138 - val_loss: 2.1536 - val_mae: 2.1536\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4602 - mae: 1.4602 - val_loss: 2.1258 - val_mae: 2.1258\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4379 - mae: 1.4379 - val_loss: 2.0955 - val_mae: 2.0955\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4783 - mae: 1.4783 - val_loss: 2.2283 - val_mae: 2.2283\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4658 - mae: 1.4658 - val_loss: 2.1143 - val_mae: 2.1143\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4357 - mae: 1.4357 - val_loss: 2.1990 - val_mae: 2.1990\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.5146 - mae: 1.5146 - val_loss: 2.0775 - val_mae: 2.0775\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4494 - mae: 1.4494 - val_loss: 2.1858 - val_mae: 2.1858\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4595 - mae: 1.4595 - val_loss: 2.1332 - val_mae: 2.1332\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4053 - mae: 1.4053 - val_loss: 2.1218 - val_mae: 2.1218\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4084 - mae: 1.4084 - val_loss: 2.0810 - val_mae: 2.0810\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4723 - mae: 1.4723 - val_loss: 2.0820 - val_mae: 2.0820\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4232 - mae: 1.4232 - val_loss: 2.1522 - val_mae: 2.1522\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4326 - mae: 1.4326 - val_loss: 2.1473 - val_mae: 2.1473\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4174 - mae: 1.4174 - val_loss: 2.1041 - val_mae: 2.1041\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4825 - mae: 1.4825 - val_loss: 2.0933 - val_mae: 2.0933\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4146 - mae: 1.4146 - val_loss: 2.1242 - val_mae: 2.1242\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4198 - mae: 1.4198 - val_loss: 2.0909 - val_mae: 2.0909\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3928 - mae: 1.3928 - val_loss: 2.1231 - val_mae: 2.1231\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3767 - mae: 1.3767 - val_loss: 2.1122 - val_mae: 2.1122\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3928 - mae: 1.3928 - val_loss: 2.0996 - val_mae: 2.0996\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3895 - mae: 1.3895 - val_loss: 2.0860 - val_mae: 2.0860\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4039 - mae: 1.4039 - val_loss: 2.0576 - val_mae: 2.0576\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4105 - mae: 1.4105 - val_loss: 2.0962 - val_mae: 2.0962\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3594 - mae: 1.3594 - val_loss: 2.1364 - val_mae: 2.1364\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3912 - mae: 1.3912 - val_loss: 2.1309 - val_mae: 2.1309\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3753 - mae: 1.3753 - val_loss: 2.1481 - val_mae: 2.1481\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3933 - mae: 1.3933 - val_loss: 2.0841 - val_mae: 2.0841\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4258 - mae: 1.4258 - val_loss: 2.0974 - val_mae: 2.0974\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4911 - mae: 1.4911 - val_loss: 2.1325 - val_mae: 2.1325\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3685 - mae: 1.3685 - val_loss: 2.1587 - val_mae: 2.1587\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4083 - mae: 1.4083 - val_loss: 2.0949 - val_mae: 2.0949\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4080 - mae: 1.4080 - val_loss: 2.0591 - val_mae: 2.0591\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3900 - mae: 1.3900 - val_loss: 2.1561 - val_mae: 2.1561\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3730 - mae: 1.3730 - val_loss: 2.1155 - val_mae: 2.1155\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4192 - mae: 1.4192 - val_loss: 2.1229 - val_mae: 2.1229\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3925 - mae: 1.3925 - val_loss: 2.0279 - val_mae: 2.0279\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4496 - mae: 1.4496 - val_loss: 2.1157 - val_mae: 2.1157\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3515 - mae: 1.3515 - val_loss: 2.0593 - val_mae: 2.0593\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3502 - mae: 1.3502 - val_loss: 2.0304 - val_mae: 2.0304\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3419 - mae: 1.3419 - val_loss: 2.0585 - val_mae: 2.0585\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3526 - mae: 1.3526 - val_loss: 2.0680 - val_mae: 2.0680\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3448 - mae: 1.3448 - val_loss: 2.0373 - val_mae: 2.0373\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3437 - mae: 1.3437 - val_loss: 2.1825 - val_mae: 2.1825\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3442 - mae: 1.3442 - val_loss: 2.1535 - val_mae: 2.1535\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3879 - mae: 1.3879 - val_loss: 2.0432 - val_mae: 2.0432\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3428 - mae: 1.3428 - val_loss: 2.0446 - val_mae: 2.0446\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3374 - mae: 1.3374 - val_loss: 2.1061 - val_mae: 2.1061\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3445 - mae: 1.3445 - val_loss: 2.0148 - val_mae: 2.0148\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2992 - mae: 1.2992 - val_loss: 2.0046 - val_mae: 2.0046\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2873 - mae: 1.2873 - val_loss: 2.0736 - val_mae: 2.0736\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2636 - mae: 1.2636 - val_loss: 2.0155 - val_mae: 2.0155\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2679 - mae: 1.2679 - val_loss: 2.0719 - val_mae: 2.0719\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3261 - mae: 1.3261 - val_loss: 2.0635 - val_mae: 2.0635\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3181 - mae: 1.3181 - val_loss: 2.0457 - val_mae: 2.0457\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2846 - mae: 1.2846 - val_loss: 2.0465 - val_mae: 2.0465\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2552 - mae: 1.2552 - val_loss: 2.0104 - val_mae: 2.0104\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2558 - mae: 1.2558 - val_loss: 2.0600 - val_mae: 2.0600\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2601 - mae: 1.2601 - val_loss: 2.0091 - val_mae: 2.0091\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2755 - mae: 1.2755 - val_loss: 2.0448 - val_mae: 2.0448\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3172 - mae: 1.3172 - val_loss: 1.9707 - val_mae: 1.9707\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3035 - mae: 1.3035 - val_loss: 2.0012 - val_mae: 2.0012\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2802 - mae: 1.2802 - val_loss: 1.9895 - val_mae: 1.9895\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2646 - mae: 1.2646 - val_loss: 1.9752 - val_mae: 1.9752\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3104 - mae: 1.3104 - val_loss: 2.0106 - val_mae: 2.0106\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3143 - mae: 1.3143 - val_loss: 2.0720 - val_mae: 2.0720\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2799 - mae: 1.2799 - val_loss: 1.9667 - val_mae: 1.9667\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2589 - mae: 1.2589 - val_loss: 2.0084 - val_mae: 2.0084\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2515 - mae: 1.2515 - val_loss: 2.0074 - val_mae: 2.0074\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2456 - mae: 1.2456 - val_loss: 2.0233 - val_mae: 2.0233\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3189 - mae: 1.3189 - val_loss: 2.0012 - val_mae: 2.0012\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2584 - mae: 1.2584 - val_loss: 1.9782 - val_mae: 1.9782\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2820 - mae: 1.2820 - val_loss: 2.0318 - val_mae: 2.0318\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2528 - mae: 1.2528 - val_loss: 2.0281 - val_mae: 2.0281\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2222 - mae: 1.2222 - val_loss: 1.9742 - val_mae: 1.9742\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2391 - mae: 1.2391 - val_loss: 2.0057 - val_mae: 2.0057\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2495 - mae: 1.2495 - val_loss: 1.9817 - val_mae: 1.9817\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2652 - mae: 1.2652 - val_loss: 1.9710 - val_mae: 1.9710\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2576 - mae: 1.2576 - val_loss: 1.9701 - val_mae: 1.9701\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2410 - mae: 1.2410 - val_loss: 1.9939 - val_mae: 1.9939\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2204 - mae: 1.2204 - val_loss: 1.9837 - val_mae: 1.9837\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2242 - mae: 1.2242 - val_loss: 2.0156 - val_mae: 2.0156\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2379 - mae: 1.2379 - val_loss: 1.9955 - val_mae: 1.9955\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2932 - mae: 1.2932 - val_loss: 1.9860 - val_mae: 1.9860\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2415 - mae: 1.2415 - val_loss: 2.0012 - val_mae: 2.0012\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2703 - mae: 1.2703 - val_loss: 1.9370 - val_mae: 1.9370\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2480 - mae: 1.2480 - val_loss: 1.9545 - val_mae: 1.9545\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2083 - mae: 1.2083 - val_loss: 2.0115 - val_mae: 2.0115\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2161 - mae: 1.2161 - val_loss: 1.9445 - val_mae: 1.9445\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2090 - mae: 1.2090 - val_loss: 1.9920 - val_mae: 1.9920\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2468 - mae: 1.2468 - val_loss: 2.0211 - val_mae: 2.0211\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.2688 - mae: 1.2688 - val_loss: 1.9534 - val_mae: 1.9534\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1937 - mae: 1.1937 - val_loss: 1.9898 - val_mae: 1.9898\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2207 - mae: 1.2207 - val_loss: 2.0435 - val_mae: 2.0435\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2014 - mae: 1.2014 - val_loss: 2.0049 - val_mae: 2.0049\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2549 - mae: 1.2549 - val_loss: 1.9909 - val_mae: 1.9909\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1817 - mae: 1.1817 - val_loss: 1.9636 - val_mae: 1.9636\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1934 - mae: 1.1934 - val_loss: 1.9653 - val_mae: 1.9653\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2222 - mae: 1.2222 - val_loss: 1.9304 - val_mae: 1.9304\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2515 - mae: 1.2515 - val_loss: 1.9223 - val_mae: 1.9223\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1964 - mae: 1.1964 - val_loss: 1.9886 - val_mae: 1.9886\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.1925 - mae: 1.1925 - val_loss: 1.9786 - val_mae: 1.9786\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.2335 - mae: 1.2335 - val_loss: 1.9977 - val_mae: 1.9977\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2178 - mae: 1.2178 - val_loss: 1.9232 - val_mae: 1.9232\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1630 - mae: 1.1630 - val_loss: 1.9629 - val_mae: 1.9629\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1707 - mae: 1.1707 - val_loss: 2.0158 - val_mae: 2.0158\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2276 - mae: 1.2276 - val_loss: 1.9786 - val_mae: 1.9786\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1740 - mae: 1.1740 - val_loss: 1.9599 - val_mae: 1.9599\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1963 - mae: 1.1963 - val_loss: 1.8953 - val_mae: 1.8953\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1860 - mae: 1.1860 - val_loss: 1.9150 - val_mae: 1.9150\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1631 - mae: 1.1631 - val_loss: 1.9236 - val_mae: 1.9236\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2249 - mae: 1.2249 - val_loss: 1.9957 - val_mae: 1.9957\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1766 - mae: 1.1766 - val_loss: 1.9512 - val_mae: 1.9512\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.2267 - mae: 1.2267 - val_loss: 1.9240 - val_mae: 1.9240\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1855 - mae: 1.1855 - val_loss: 1.9003 - val_mae: 1.9003\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1890 - mae: 1.1890 - val_loss: 1.9160 - val_mae: 1.9160\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1390 - mae: 1.1390 - val_loss: 1.9733 - val_mae: 1.9733\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1630 - mae: 1.1630 - val_loss: 1.9413 - val_mae: 1.9413\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1450 - mae: 1.1450 - val_loss: 1.9044 - val_mae: 1.9044\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1466 - mae: 1.1466 - val_loss: 1.9136 - val_mae: 1.9136\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2156 - mae: 1.2156 - val_loss: 1.9452 - val_mae: 1.9452\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1190 - mae: 1.1190 - val_loss: 1.9182 - val_mae: 1.9182\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1225 - mae: 1.1225 - val_loss: 1.9528 - val_mae: 1.9528\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1430 - mae: 1.1430 - val_loss: 1.9245 - val_mae: 1.9245\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1178 - mae: 1.1178 - val_loss: 1.9429 - val_mae: 1.9429\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1072 - mae: 1.1072 - val_loss: 1.9071 - val_mae: 1.9071\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1061 - mae: 1.1061 - val_loss: 1.9177 - val_mae: 1.9177\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1040 - mae: 1.1040 - val_loss: 1.9242 - val_mae: 1.9242\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1413 - mae: 1.1413 - val_loss: 1.9198 - val_mae: 1.9198\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1523 - mae: 1.1523 - val_loss: 1.9681 - val_mae: 1.9681\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1557 - mae: 1.1557 - val_loss: 1.8929 - val_mae: 1.8929\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1128 - mae: 1.1128 - val_loss: 1.9426 - val_mae: 1.9426\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0755 - mae: 1.0755 - val_loss: 1.9279 - val_mae: 1.9279\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.0924 - mae: 1.0924 - val_loss: 1.8599 - val_mae: 1.8599\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.1169 - mae: 1.1169 - val_loss: 1.8989 - val_mae: 1.8989\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1121 - mae: 1.1121 - val_loss: 1.8768 - val_mae: 1.8768\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1007 - mae: 1.1007 - val_loss: 1.9121 - val_mae: 1.9121\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1092 - mae: 1.1092 - val_loss: 1.9420 - val_mae: 1.9420\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1189 - mae: 1.1189 - val_loss: 1.9137 - val_mae: 1.9137\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51c01f0370>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "outputId": "c4dfb19a-4317-4491-a577-ba4ddcdba84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "mae = np.abs(errors)/n\n",
        "\n",
        "plt.plot(errors, mae, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "outputId": "0b36469f-160c-428e-9381-712bada4905c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 2s 18ms/step - loss: 6.8110 - mean_squared_logarithmic_error: 6.8110 - val_loss: 5.2567 - val_mean_squared_logarithmic_error: 5.2567\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.2322 - mean_squared_logarithmic_error: 4.2322 - val_loss: 3.1795 - val_mean_squared_logarithmic_error: 3.1795\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.4831 - mean_squared_logarithmic_error: 2.4831 - val_loss: 1.7830 - val_mean_squared_logarithmic_error: 1.7830\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3754 - mean_squared_logarithmic_error: 1.3754 - val_loss: 0.9614 - val_mean_squared_logarithmic_error: 0.9614\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7596 - mean_squared_logarithmic_error: 0.7596 - val_loss: 0.5152 - val_mean_squared_logarithmic_error: 0.5152\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4276 - mean_squared_logarithmic_error: 0.4276 - val_loss: 0.2903 - val_mean_squared_logarithmic_error: 0.2903\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2615 - mean_squared_logarithmic_error: 0.2615 - val_loss: 0.1813 - val_mean_squared_logarithmic_error: 0.1813\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1810 - mean_squared_logarithmic_error: 0.1810 - val_loss: 0.1273 - val_mean_squared_logarithmic_error: 0.1273\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1360 - mean_squared_logarithmic_error: 0.1360 - val_loss: 0.0984 - val_mean_squared_logarithmic_error: 0.0984\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1121 - mean_squared_logarithmic_error: 0.1121 - val_loss: 0.0804 - val_mean_squared_logarithmic_error: 0.0804\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0957 - mean_squared_logarithmic_error: 0.0957 - val_loss: 0.0687 - val_mean_squared_logarithmic_error: 0.0687\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0854 - mean_squared_logarithmic_error: 0.0854 - val_loss: 0.0606 - val_mean_squared_logarithmic_error: 0.0606\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0775 - mean_squared_logarithmic_error: 0.0775 - val_loss: 0.0551 - val_mean_squared_logarithmic_error: 0.0551\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0716 - mean_squared_logarithmic_error: 0.0716 - val_loss: 0.0510 - val_mean_squared_logarithmic_error: 0.0510\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0666 - mean_squared_logarithmic_error: 0.0666 - val_loss: 0.0468 - val_mean_squared_logarithmic_error: 0.0468\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0626 - mean_squared_logarithmic_error: 0.0626 - val_loss: 0.0432 - val_mean_squared_logarithmic_error: 0.0432\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0586 - mean_squared_logarithmic_error: 0.0586 - val_loss: 0.0406 - val_mean_squared_logarithmic_error: 0.0406\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0554 - mean_squared_logarithmic_error: 0.0554 - val_loss: 0.0380 - val_mean_squared_logarithmic_error: 0.0380\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0521 - mean_squared_logarithmic_error: 0.0521 - val_loss: 0.0360 - val_mean_squared_logarithmic_error: 0.0360\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0495 - mean_squared_logarithmic_error: 0.0495 - val_loss: 0.0342 - val_mean_squared_logarithmic_error: 0.0342\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0470 - mean_squared_logarithmic_error: 0.0470 - val_loss: 0.0325 - val_mean_squared_logarithmic_error: 0.0325\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0449 - mean_squared_logarithmic_error: 0.0449 - val_loss: 0.0314 - val_mean_squared_logarithmic_error: 0.0314\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0428 - mean_squared_logarithmic_error: 0.0428 - val_loss: 0.0301 - val_mean_squared_logarithmic_error: 0.0301\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0411 - mean_squared_logarithmic_error: 0.0411 - val_loss: 0.0291 - val_mean_squared_logarithmic_error: 0.0291\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0395 - mean_squared_logarithmic_error: 0.0395 - val_loss: 0.0285 - val_mean_squared_logarithmic_error: 0.0285\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0381 - mean_squared_logarithmic_error: 0.0381 - val_loss: 0.0277 - val_mean_squared_logarithmic_error: 0.0277\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0368 - mean_squared_logarithmic_error: 0.0368 - val_loss: 0.0270 - val_mean_squared_logarithmic_error: 0.0270\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0356 - mean_squared_logarithmic_error: 0.0356 - val_loss: 0.0263 - val_mean_squared_logarithmic_error: 0.0263\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0344 - mean_squared_logarithmic_error: 0.0344 - val_loss: 0.0258 - val_mean_squared_logarithmic_error: 0.0258\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0334 - mean_squared_logarithmic_error: 0.0334 - val_loss: 0.0256 - val_mean_squared_logarithmic_error: 0.0256\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0325 - mean_squared_logarithmic_error: 0.0325 - val_loss: 0.0254 - val_mean_squared_logarithmic_error: 0.0254\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0318 - mean_squared_logarithmic_error: 0.0318 - val_loss: 0.0249 - val_mean_squared_logarithmic_error: 0.0249\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0309 - mean_squared_logarithmic_error: 0.0309 - val_loss: 0.0255 - val_mean_squared_logarithmic_error: 0.0255\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0302 - mean_squared_logarithmic_error: 0.0302 - val_loss: 0.0253 - val_mean_squared_logarithmic_error: 0.0253\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0295 - mean_squared_logarithmic_error: 0.0295 - val_loss: 0.0248 - val_mean_squared_logarithmic_error: 0.0248\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0289 - mean_squared_logarithmic_error: 0.0289 - val_loss: 0.0239 - val_mean_squared_logarithmic_error: 0.0239\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0285 - mean_squared_logarithmic_error: 0.0285 - val_loss: 0.0241 - val_mean_squared_logarithmic_error: 0.0241\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0280 - mean_squared_logarithmic_error: 0.0280 - val_loss: 0.0241 - val_mean_squared_logarithmic_error: 0.0241\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0275 - mean_squared_logarithmic_error: 0.0275 - val_loss: 0.0239 - val_mean_squared_logarithmic_error: 0.0239\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0271 - mean_squared_logarithmic_error: 0.0271 - val_loss: 0.0237 - val_mean_squared_logarithmic_error: 0.0237\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0267 - mean_squared_logarithmic_error: 0.0267 - val_loss: 0.0233 - val_mean_squared_logarithmic_error: 0.0233\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0263 - mean_squared_logarithmic_error: 0.0263 - val_loss: 0.0231 - val_mean_squared_logarithmic_error: 0.0231\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0260 - mean_squared_logarithmic_error: 0.0260 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0257 - mean_squared_logarithmic_error: 0.0257 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0254 - mean_squared_logarithmic_error: 0.0254 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0252 - mean_squared_logarithmic_error: 0.0252 - val_loss: 0.0229 - val_mean_squared_logarithmic_error: 0.0229\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0249 - mean_squared_logarithmic_error: 0.0249 - val_loss: 0.0224 - val_mean_squared_logarithmic_error: 0.0224\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0246 - mean_squared_logarithmic_error: 0.0246 - val_loss: 0.0219 - val_mean_squared_logarithmic_error: 0.0219\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0244 - mean_squared_logarithmic_error: 0.0244 - val_loss: 0.0222 - val_mean_squared_logarithmic_error: 0.0222\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0242 - mean_squared_logarithmic_error: 0.0242 - val_loss: 0.0217 - val_mean_squared_logarithmic_error: 0.0217\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0241 - mean_squared_logarithmic_error: 0.0241 - val_loss: 0.0219 - val_mean_squared_logarithmic_error: 0.0219\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0237 - mean_squared_logarithmic_error: 0.0237 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0235 - mean_squared_logarithmic_error: 0.0235 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0233 - mean_squared_logarithmic_error: 0.0233 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0231 - mean_squared_logarithmic_error: 0.0231 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0229 - mean_squared_logarithmic_error: 0.0229 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0228 - mean_squared_logarithmic_error: 0.0228 - val_loss: 0.0209 - val_mean_squared_logarithmic_error: 0.0209\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0226 - mean_squared_logarithmic_error: 0.0226 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0224 - mean_squared_logarithmic_error: 0.0224 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0223 - mean_squared_logarithmic_error: 0.0223 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0220 - mean_squared_logarithmic_error: 0.0220 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0218 - mean_squared_logarithmic_error: 0.0218 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0217 - mean_squared_logarithmic_error: 0.0217 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0216 - mean_squared_logarithmic_error: 0.0216 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0213 - mean_squared_logarithmic_error: 0.0213 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0212 - mean_squared_logarithmic_error: 0.0212 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0212 - mean_squared_logarithmic_error: 0.0212 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0209 - mean_squared_logarithmic_error: 0.0209 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0209 - mean_squared_logarithmic_error: 0.0209 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0208 - mean_squared_logarithmic_error: 0.0208 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0207 - mean_squared_logarithmic_error: 0.0207 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0204 - mean_squared_logarithmic_error: 0.0204 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0202 - mean_squared_logarithmic_error: 0.0202 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0201 - mean_squared_logarithmic_error: 0.0201 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0201 - mean_squared_logarithmic_error: 0.0201 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0200 - mean_squared_logarithmic_error: 0.0200 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0198 - mean_squared_logarithmic_error: 0.0198 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0196 - mean_squared_logarithmic_error: 0.0196 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0189 - mean_squared_logarithmic_error: 0.0189 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0180 - val_mean_squared_logarithmic_error: 0.0180\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0186 - mean_squared_logarithmic_error: 0.0186 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0180 - val_mean_squared_logarithmic_error: 0.0180\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0180 - mean_squared_logarithmic_error: 0.0180 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0181 - mean_squared_logarithmic_error: 0.0181 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0176 - mean_squared_logarithmic_error: 0.0176 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0180 - val_mean_squared_logarithmic_error: 0.0180\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0173 - val_mean_squared_logarithmic_error: 0.0173\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0168 - val_mean_squared_logarithmic_error: 0.0168\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0170 - val_mean_squared_logarithmic_error: 0.0170\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0173 - val_mean_squared_logarithmic_error: 0.0173\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0168 - val_mean_squared_logarithmic_error: 0.0168\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0169 - val_mean_squared_logarithmic_error: 0.0169\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0173 - val_mean_squared_logarithmic_error: 0.0173\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0171 - val_mean_squared_logarithmic_error: 0.0171\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0170 - val_mean_squared_logarithmic_error: 0.0170\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0173 - val_mean_squared_logarithmic_error: 0.0173\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0155 - mean_squared_logarithmic_error: 0.0155 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0154 - mean_squared_logarithmic_error: 0.0154 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0154 - mean_squared_logarithmic_error: 0.0154 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f514ef8b160>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-",
        "outputId": "c5d93023-bb57-4524-9f96-9e66258a7344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "from math import log\n",
        "actual_outputs = np.arange(0, 51)\n",
        "n = len(actual_outputs)\n",
        "estimated_outputs = np.zeros(n)\n",
        "\n",
        "msle = np.square(np.log(estimated_outputs + 1) - np.log(actual_outputs +1))\n",
        "\n",
        "plt.plot(actual_outputs, msle, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual ouputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEHCAYAAACp9y31AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmk0lEQVR4nO3de5zcdX3v8dd7N7sbwiaGEFguAQN4KwQEsygIxQQRYkFRsAqopVZPausFW63VWg9ij+fYllo9tac9qSLeDik3kXILEQkBlVtIgAiIt6AIglyWzSZkr5/zx++3YbI7M/vb2ZnZnfm9n49HHjvzm9/l+2WXz373870pIjAzs/xome4CmJlZfTnwm5nljAO/mVnOOPCbmeWMA7+ZWc448JuZ5cysWt1Y0kXAacCTEbGk4PiHgA8Aw8C1EfHxie61cOHCWLx4cUXl2LZtG7vvvntF1zYq1zkfXOd8mEqdN2zY8FRE7DX2eM0CP3Ax8GXgG6MHJC0HTgdeGRH9kvbOcqPFixdz9913V1SIdevWsWzZsoqubVSucz64zvkwlTpLeqTY8ZqleiJiPfDMmMN/Bnw+IvrTc56s1fPNzKw41XLmrqTFwDWjqR5Jm4DvAiuAHcDHIuKuEteuBFYCdHV1LV29enVFZejr66Ozs7OiaxuV65wPrnM+TKXOy5cv3xAR3WOP1zLVU8wsYAFwDHA0cKmkg6PIb5+IWAWsAuju7o5K/9Txn4b54Drng+tcHfUe1fMocGUk7gRGgIV1LoOZWa7VO/BfBSwHkPQyoB14qs5lMDPLtVoO57wEWAYslPQocD5wEXCRpM3AAHBusTSPmVneDW58gME16+nu6WX77Q/RdsoJtB11aFXuXbPAHxFnl/joXbV6pplZMxjc+AADV94Ag0MIiJ7e5D1UJfjXu3PXzMxSo6366OlF8+ftbNUPrlkPg0NjTh5icM16B34zs0ZV2KqHtFV/xfUM3rOZ6Oktek2p45PlwG9mVmPFWvZFW/VDw8RPt8CsWTA0NO4+mj+vKuXxIm1mZjU02rIfba1HTy8Dl19ftvXefuYKaBvTLm+bRdspJ1SlTG7xm5lVSeaW/fBwyXto/rydefzBNesZ6emlpSD/Xw0O/GZmVVAqZ89Q6SBP26xdfykUtOrbjjqUtqMOrcnMXQd+M7NJmkzOvpTC68aO6qk1B34zs0moZst+tFVfbw78ZmYlNHrLvhQHfjOzIpqhZV+Kh3OamRVRScu+/YwVO8faj76fSQF/lFv8ZpZ7Y1M6s5YdU36WbIO07Etxi9/Mcq3YBKvBq24seX4jtexLcYvfzHJjXMv+DcczeN268SkdgI52GBlp6JZ9KW7xm1kuFG3ZX3YdbNte/IL+gYZv2ZfiFr+ZNZ2iwzCvX1e8ZS9Bkf2gRpdOaIZAP5YDv5k1laLDMC+9tmhwT06IsksnNKNabr14EXAa8GRELBnz2UeBC4G9IsJ77ppZRTJPsCqzw+tMnGBVa5MK/JL2AA6IiPsynH4x8GXgG2PucQBwMvCryTzbzKxQ0Zb9ZdclHbKlNPgwzGqZsHNX0jpJ8yQtAO4B/kPSFya6LiLWA88U+eifgY8D3mTdzCpWtGVfJug3wzDMalGU+RMIQNLGiDhK0vtIWvvnS7ovIo6Y8ObSYuCa0VSPpNOBEyPiPElbgO5SqR5JK4GVAF1dXUtXr149mXrt1NfXR2dnZ0XXNirXOR/yVOcFjz3FoocfpX3HAAOz23mmaw/2eeQJVOTcAEZaWmgt+CUw3NLCliWLeWa/hXUrc7VM5fu8fPnyDRHRPfZ4llTPLEn7Am8HPlXR0wFJc4C/IUnzTCgiVgGrALq7u6PS9ahrsZb1TOc650Ne6jy48QEGbrpnZ+u+Y8cA+z7yRMnzW+bPo2NMzn63U07giAZt2U/XevwXAGuA2yLiLkkHAz+t4FmHAAcB90oCWATcI+nVEfHbCu5nZk1m3ASrE49l8Ppbig/D3K0jWTvHOftJKxv4JbWSpHd2pnUi4hfAmZN9UETcD+xdcO8tlEn1mFm+FOusHbxyTekLnu+n/R2n5Wo0TrWUDfwRMSzpbJIO2UmRdAmwDFgo6VHg/Ij4akWlNLOmUWwIZttRh3qCVR1lSfX8QNKXgf8Eto0ejIh7yl0UEWdP8PniLAU0s+ZRdAjm5dczeOudRG9f8YtyOMGq1rIE/iPTr58tOBbAiVUvjZk1taJDMIeHicd/lyyK1j8w7prCCVYjPb20OKUzZRMG/ohYXo+CmFlzKbYSZsk17iNof8vJu/w1AIzrrM3LSKZamzDwS3oRcD4w+nfVLcBnI+K5WhbMzBpX0Y7ay64ref5ovh5wZ20dZEn1XARsJhnHD/Bu4GvAGbUqlJk1juLr5ZQYgtneluTsS+Tr3VlbH1kC/yERUTh88wJJm2pUHjNrIJNeL2dg0EMwZ4Asgf95ScdHxG0Ako4Dnq9tscysEVSyXo5b9dMvS+B/P/CNNNcP8Cxwbu2KZGYz0fgNyV9T0YbkNv2yzNx9d0S8UtI8gIgo8502s2ZUtLP2qrUlz8/jGveNJMvM3ePT1w74ZjlQtLP2hhKdtU28IXkzy5Lq2SjpauAydp25e2XNSmVm02LSnbX9A+6sbUBZAv9s4Gl2nakbgAO/WZNxZ20+ZMnxPx0RH6tTecysTsZ11r7+te6szYmyWy9GxDBwXJ3KYmZ1MprSGQ300dPL4BU3lDzf2xY2lyypnk3O8Zs1l6IpHYCONhgpPrPWKZ3m4Ry/WZPbNaUzl5ZXHFI6pdPvmbV5kGV1zvfUoyBmVn3jR+lsZfj2Td7cJOdKBn5Jl0bE29PXfx8Rf13w2Y0RUXbTdEkXAacBT0bEkvTYPwJvAgaAnwPviYieKdfCzIqPvy+1q9XsDhgacmdtTpXr3H1pwes3jPlsrwz3vhhYMebYWmBJuofvw8AnM9zHzCZQrLN24NJrS+9q9fwOd9bmWLlUz/i/A7N9lpwQsV7S4jHHbix4ezvwtonuY2YTK9pZWySVM8opnXwrF/jnSDqK5K+C3dLXSv/tVoVn/wnJPr5FSVoJrATo6upi3bp1FT2kr6+v4msblevc3BY89hSLHn6U7h0DPLNuE795yf4c1NOLipwbwEhLC60Fk7CGW1rYcuBCnmnA/155+j6PqkWdFSVaBZJuLndhli0Z0xb/NaM5/oLjnwK6gTOiVAEKdHd3x9133z3RaUXlcas217l5je2snUizLZaWl+9zoanUWdKGiOgee7xki79We+1K+mOSTt/XZwn6ZvYCj7+3asgyjr9qJK0APg68LiK21/PZZo1m3JIKxx7l8fdWFTUL/JIuAZYBCyU9SrJh+yeBDmCtJIDbI+L9tSqDWaMquv799beUPN+dtTYZNQv8EXF2kcNfrdXzzJpJyZTObh0wNOzx9zYlEwZ+SW8Fvh8Rz6Xv5wPLIuKq2hbNLB/GpnRaDn9Z6ZTO8/07UzojPb20OKVjFcjS4j8/Ir4z+iYieiSdD1xVs1KZ5USxlM7wrXcng6aLDH0oTOnkcYSLVUfZZZnLnFPXTmGzZlUypTO7I1n/vpBTOlYlWQL43ZK+APxr+v4DwIbaFcmsOY1L6bzsoEwpHY/SsWrLEvg/BHyaF2bZriUJ/maWUdGUzp33epVMmxZZlmXeBnyiDmUxa1qDa27xKpk2Y5RblvmLEfERSf9FkW6miHhzTUtm1oDGTbp6w/EogujZWvyC53c4pWN1V67F/83064X1KIhZoys66eqy65IPW1tgeGTcNU7p2HQot1bPhvRr6emCZrZTyRE6u+9G26knMvidNU7p2IyQZQLXacDfAS9OzxcQETGvxmUzm7HGpXRO/v3SI3S2PU/7qw5DklM6NiNkGdXzReAM4H6vpmlWKqVzbcnzR3e5ckrHZoosE7h+DWx20DdLFN/timTClSddWQPI0uL/OHCdpFuA/tGDEfGFmpXKbIYotoF5yZTO4JBH6FhDyBL4Pwf0AbOB9toWx2zmKJbSGbi0fErH6RxrBFkC/35jt040y4OSG5i3tkKLPELHGlaWHP91kk6ueUnMZpiSKZ3hYdrPWLGz01bz59F+xgq39K1hZGnx/xnwMUn9wCAezmlNaNwCai8/2OvoWNOasMUfEXMjoiUidouIeen7CYO+pIskPSlpc8GxBZLWSvpp+nWPqVbAbKpGc/mjLfzo6WX4jk3JOjqzWnc92SkdawJZUj1I2l/SayWdMPovw2UXAyvGHPsEcFNEvBS4CS/+ZjNAqRm36min/cw3OqVjTSfLzN2/B94BPAAMp4cDWF/uuohYL2nxmMOnk2zADvB1YB3w15lLazZF42bcLntNyVx+9PQ6pWNNSRPNy5L0E+CIiOgve2LxaxcD14yOCpLUExHz09cCnh19X+TalcBKgK6urqWrV6+e7OMB6Ovro7Ozs6JrG5XrXNyCx55i8eYttI68sFja6E+/ipzfP7ud+5YdWbUyVpu/z/kwlTovX758Q0R0jz2epXP3F0AbBZO3qiEiQlLJ3zoRsQpYBdDd3R2V7i2ax31JXefitn/+34mRXVfIFEBHO4yMjBueOff0k1k2g1v7/j7nQy3qXG49/n8haRBtBzZJuoldZ+5+uILnPSFp34h4XNK+wJMV3MOsIiWHZ/YPeMat5Uq5Fv/d6dcNwNVjPqt03Z6rgXOBz6dfv1vhfczK2iWX/6K5sHBByXM9PNPyptx6/F8HkHReRHyp8DNJ5010Y0mXkHTkLpT0KHA+ScC/VNJ7gUeAt1dedLPixi218NxWeG4r7L0nPPucZ9xa7mUZznlukWN/PNFFEXF2ROwbEW0RsSgivhoRT0fE6yPipRFxUkQ8M+kSm02g5PDMgUHPuDWjfI7/bOAc4CBJhameuYADts0Ioymd7p5ett/+ELNe92oPzzSbQLkc/w+Bx4GFwD8VHN8K3FfLQpllUZjSEemGKN/9XsnzR1v6ZnlXLsf/CEke/tj6Fccsu5J73M7ugOFh5/LNSiiZ45d0W/p1q6Tegn9bJZUYF2dWPyWHZ+7ody7frIxyLf7j069z61ccs+LGrZ75ssVePdOsQmVH9UhqlfRQvQpjVkzR1TPvvA/mzPbqmWYVKBv4I2IY+ImkA+tUHrNxSg7PbGvbuXpm4JSOWVZZ1urZA/ixpDuBbaMHI+LNNSuVWYEswzPzuIaLWaWyBP5P17wUZqlxSy0smF/yXA/PNKvMhIE/Im6pR0HMSi61sM9e8PSzHp5pViUTLtkg6RhJd0nqkzQgadjDOa0WSubyPTzTrKqypHq+DJwFXAZ0A38EvKyWhbL8iQgvtWBWJ1kCPxHxM0mt6Sifr0naCHyytkWzZrZLLn9eJ+w+p+S5zuWbVVeWwL9dUjvJZiz/QLJ+T6ZN2s2KGZfL7+2D3j504H7E4086l29WY1kC+LuBVuCDJMM5DwDOrGWhrLmVXGOnt8+5fLM6yDKq55H05fPABbUtjuWBc/lm02vCwC/pfsZvtfgcydaM/yMinp7sQyX9BfC+9L73A++JiB2TvY/NfOPG5c/rLHmuc/lm9ZEl1XM9cC3wzvTff5EE/d8CF0/2gZL2Bz4MdEfEEpI00lmTvY/NfOPW2HluK/Hrx2HRPtA2ps3hXL5Z3WTp3D0pIl5V8P5+SfdExKskvWsKz91N0iAwB3iswvvYDFZyXH7fdtrOWLHLapttp5zgFI9ZnWQJ/K2SXh0RdwJIOpqklQ5QpIeuvIj4jaQLgV+R9BvcGBE3TvY+NvM5l282MymKrGe+ywlJoL8I6AQE9ALvBR4ATo2ISyf1QGkP4ArgHUAPycSwyyPiW2POWwmsBOjq6lq6evXqyTxmp76+Pjo7S+eVm9G01zmCfX75OIsefhQV+bh/djv3LTuyqo+c9jpPA9c5H6ZS5+XLl2+IiO6xxycM/DtPlF4EEBHPVVSCF+7zh8CKiHhv+v6PgGMi4s9LXdPd3R133313Rc/L46qN9a7z2MlY0dEOv3sGLdqHeOKpcePyazFE09/nfHCdJ0dS0cCfZVTPi4DzgRPS97cAn53CL4BfAcdImkOS6nk9SWexNaCik7GA1qOPoOOMUxja9KBz+WYzTJYc/0XAZuDt6ft3A18DzqjkgRFxh6TLgXtI+gg2AqsquZdNv1IduCM/3YIk5/LNZqAsgf+QiCicqXuBpE1TeWhEnE/yV4Q1uHIduGY2M2UJ/M9LOj4ibgOQdBxJisZyZtyG5y8/uOS5noxlNnNlCfzvB74x2rkLPAucW7si2Uw0Lpff08vwHZugcw7sGIAhL6xm1igmnLkbEfdGxCuBI4AjIuIo4MSal8xmlJKTsWbNov1ML6xm1kgyrccPEBGFSdu/BL5Y9dLYjOXJWGbNo9J19YvNy7EmFUPD0NFW9DPn8s0aT+YW/xjZZn1ZQxo3IWvWLOgfhBbBSMG33rl8s4ZUMvBL2krxAC9gt5qVyKZVyQlZx76K1gP382QssyZQMvBHxNx6FsRmhpITsh78GbNPP8mB3qwJeO9c24UnZJk1Pwd+22mkpxdai/9IuBPXrHlU2rlrTaCwE5fOOTAwCAhaW2F4+IUT3Ylr1lQc+HNqbCcufdsBaHvj69C8ue7ENWtilYzqASAi/Ld/AyvViTv0o43M+cT7HejNmtiEo3ok/R3wOPBNkqGc7wT2rUvprGbciWuWX1k6d98cEf8nIrZGRG9E/Btweq0LZrUz8kwPtLgT1yyvsuT4t0l6J7CaJPVzNrCtpqWyqiraidsiaGmFIXfimuVNlhb/OSS7bz2R/vvD9Jg1gNFO3J0pnL7tMDBI28nH037mG72qplkOTdjij4gtVDm1I2k+8BVgCclfEX8SET+q5jMsUbIT94fuxDXLqwlb/JJeJukmSZvT90dI+tspPvdLwA0R8QrglcCDU7yfleBOXDMbK0uq5z+ATwKDABFxH3BWpQ9Md/I6Afhqer+BiOip9H5WWgwMwqzif9S5E9csvxRRfoVlSXdFxNGSNqa7byFpU0QcWdEDpSOBVcADJK39DcB5EbFtzHkrgZUAXV1dS1evXl3J4+jr66Ozs7OiaxvNgseeYtHDj9K+Y4CBjjaipYWO5/sJiZaC7/NwSwtblizmmf0WTmNpqytP3+dRrnM+TKXOy5cv3xAR3WOPZxnV85SkQ0gnc0l6G8m4/krNAl4FfCgi7pD0JeATwKcLT4qIVSS/IOju7o5ly5ZV9LB169ZR6bWNZHDjAwzcdM/OfH5H/yAArcctpXXRvrvMxN3tlBM4osly+3n5PhdynfOhFnXOEvg/QBKAXyHpN8AvSSZxVepR4NGIuCN9fzlJ4LcpKLmc8o9/yuw3vd6duGa2U9nAL6kV+POIOEnS7kBLRGydygMj4reSfi3p5RHxE+D1JGkfmwJ34ppZVmUDf0QMSzo+fV3NSVsfAr4tqR34BfCeKt47n3afA9u2jzvsTlwzGytLqmejpKuByyiYsRsRV1b60IjYBIzrcLDKDN3/E9g+Puh7Jq6ZFZMl8M8GngZOLDgWQMWB36ZmlyUY5syG7TtoefH+tC5dwtD3f8RITy8tXk7ZzErIMnPXaZgZZNw6+tt3gERr9+G0H30E7a9+ZS5HPphZdhMGfkmzgfcCh5G0/gGIiD+pYbmshKKjdyIYuumHtB99xPQUyswaSpaZu98E9gFOAW4BFgFTGtljlfPoHTObqiyB/yUR8WlgW0R8HTgVeE1ti2Ulze4oetijd8wsqyyBfzD92iNpCfAiYO/aFclKGVh7G+zoB2nXDzx6x8wmIcuonlWS9iBZUuFqoBP47zUtlQFjRu/M7oAd/cxaejg6+ECG1t7qzdDNrCJZRvV8JX15C3BwbYtjo8aN3tnRDy1CBx9I+9LDaF962PQW0MwaVpZRPUVb9xHx2eoXx0YVHb0zEgytvdVB38ymJNOeuwWvZwOn4Y1Tas6jd8ysVrKkev6p8L2kC4E1NSuRJXabDc/vGHfYo3fMbKqyjOoZaw7JWH6rkcENm5Og79E7ZlYDWXL895NuwgK0AnsBzu/XyNDmhxm4/HpaXvJiWo88lKHv/cCjd8ysqrLk+E8reD0EPBER43f8sIrtMmwTYMF8Zr/7raijnfbuw6e3cGbWdLKkerYW/HsemCdpwei/mpYuB0aHbe7Sabu1j6EHfjZ9hTKzppalxX8PcADwLCBgPvCr9LPAY/unpOiwzcEhBtesd1rHzGoiS4t/LfCmiFgYEXuSpH5ujIiDIsJBf4o8bNPM6i1L4D8mIq4bfRMR1wOvneqDJbVK2ijpmqneq1HF8zugpfi3wMM2zaxWsgT+xyT9raTF6b9PAY9V4dnnkeOJYDE0xI5vfgdiBFpbd/3QwzbNrIay5PjPBs4HvpO+X58eq5ikRSTLO38O+Mup3KuR7DJ6p20WDA7R8Y5TCbTzuIdtmlmtKSImPmv05GSVzp6YzEXF73M58L+AucDHIuK0IuesBFYCdHV1LV29enVFz+rr66Ozs3MKpa2OBY89xeLNW2gdGdl5bETil4cfxDP7Lazqs2ZKnevJdc4H13lyli9fviEiusceL9niTxdnuzQiHpLUAVwPvBIYlnRORHyvkoJIOg14MiI2SFpW6ryIWAWsAuju7o5K95CdKfvPbv/8vxMFQR+gJYKX/Oop5pzztqo+a6bUuZ5c53xwnaujXI7/HcBP0tfnpufuDbwO+J9TeOZxwJslbQFWAydK+tYU7tcQPHrHzGaKcoF/oCClcwpwSUQMR8SDZOsbKCoiPhkRiyJiMXAW8P2IeFel92sUmrt78eMevWNmdVYu8PdLWiJpL2A5cGPBZ3NqW6zmEtueH5fmATx6x8ymRbnAfx5wOfAQ8M8R8UsASX8AbKzGwyNiXbGO3WYSw8Ps+PZV0D/ArJOO29nC1/x5tJ+xwqN3zKzuSqZsIuIO4BVFjl8HXDf+Citm4OqbGPnFr+l4x6nMOuowOOm46S6SmeVcxbl6K23sapstrzgkCfpmZjNAJRuxWBnFVtsc+fkjDG58YBpLZWb2Agf+Kiu32qaZ2UyQKdUj6bXA4sLzI+IbNSpTQ/N4fTOb6bJsvfhN4BBgEzCcHg7Agb+Y9jYYGBx32OP1zWymyNLi7wYOner6PHkwuGFzEvRbWqBw3L7H65vZDJIlx78Z2KfWBWl0I7/9HQNX3UjLwQfQduYKj9c3sxkrS4t/IfCApDuB/tGDEfHmmpWqwcSOfnZ86yo0u4OOs99Ey9xO2pcume5imZkVlSXwf6bWhWhUY8frz1p+LC1z87VkrJk1ngkDf0TcUo+CNJrR8fqFQzeHbruLlr33dFrHzGa0CXP8ko6RdJekPkkDkoYl5X5sosfrm1mjytK5+2WSrRZ/CuwGvA/411oWqhF4vL6ZNapMM3cj4mdAa7oe/9eAFbUtVgNobyt62OP1zWymy9K5u11SO7BJ0j8Aj5PzpR6G7nvI4/XNrGFlCeDvTs/7ILANOAA4s5aFmslGenrpv3INLQfs6/H6ZtaQsozqeUTSbsC+EXHBVB8o6QCS5R66SJZ+WBURX5rqfeshRkbo/89rYWSEjrNOo2XPPTxe38waTpa1et4EXAi0AwdJOhL47BQmcA0BH42IeyTNBTZIWhsRM3Ld4l3G6s/ugB39tL/tjbTsucd0F83MrCJZUj2fAV4N9ABExCbgoEofGBGPR8Q96eutwIPA/pXer5bGra2/ox8koiXXXRxm1uA00dprkm6PiGMkbYyIo9Jj90XEEVN+uLQYWA8siYjeMZ+tBFYCdHV1LV29enVFz+jr66Ozs7LZtEes20THjoFxx/tnt3PfsiMrumc9TKXOjcp1zgfXeXKWL1++ISK6xx7PMqrnx5LOAVolvRT4MPDDikpRQFIncAXwkbFBHyAiVgGrALq7u2PZsmUVPWfdunVUeu22G+4serxjx0DF96yHqdS5UbnO+eA6V0eWnMWHgMNIFmi7BOgFPjKVh0pqIwn6346IK6dyr1oqNSbfY/XNrJFNGPgjYntEfCoijo6I7vT1jkofKEnAV4EHI+ILld6nHmad+NrxBz1W38waXMlUj6Sry104hVE9x5HMDbhf0qb02N9ExHUV3q9m4vEnkxedc6BvO5o/j7ZTTvBYfTNraOVy/McCvyZJ79wBqBoPjIjbqnWvWhr++a8Y+tE9zDpuKR1vev10F8fMrGrKBf59gDeQLNB2DnAtcElE/LgeBZtOMTBA/xXXoz3n0+60jpk1mZKBPyKGgRuAGyR1kPwCWCfpgoj4cr0KWC+7TNRKN0yf/adnoxKLsZmZNaqywznTgH8qSdBfDPxv4Du1L1Z9jdtUZWAQWsRIz1Zap7doZmZVV65z9xvAEuA64IKI2Fy3UtVZ0U1VRoLBNevdkWtmTadci/9dJKtxngd8OBmFCSQdsxERTTOY3ZuqmFmelMvx52ZBGs2fVzTIe6KWmTWj3AT3cmad5IlaZpYfWdbqaX6/ezb5uvsc2OaJWmbW3HIf+Id//TiD6+9k1tFH0HGmtxI2s+aX61RPDA3Tf/n1aO7utJ+6fLqLY2ZWF7kO/IM3/4h44ina33oymt0x3cUxM6uL3KV6dpmhC+jA/Zj1ey+Z5lKZmdVPrlr847ZSJFmBc3DjjNzu18ysJvIV+IvN0B0cSo6bmeVErgK/Z+iameUo8EcEdLQX/cwzdM0sT3IT+AfX3gb9A9AyZg8Yz9A1s5yZllE9klYAXwJaga9ExOer/YzR0TvdPb1sW3cv7OhnVvfh6KADGVp7K9HT6xm6ZpZLdQ/8klqBfyXZ3etR4C5JV0dE1YbWFK6vL4Ad/SChgw6kfelhtC89rFqPMjNrONOR6nk18LOI+EVEDACrgdOr+YCio3ciGFp7azUfY2bWkKYj1bM/ySbuox4FXjP2JEkrgZUAXV1drFu3LvMDunt6i+7mPtLTO6n7NKq+vr5c1LOQ65wPrnN1zNiZuxGxClgF0N3dHcuWLct87fbbHyo6RLNl/jwmc59GtW7dulzUs5DrnA+uc3VMR6rnN8ABBe8Xpceqpu2UE6BtzO80j94xMwOmp8V/F/BSSQeRBPyzgHOq+YDRUTqDa9Yz0tNLi0fvmJntVPfAHxFDkj4IrCEZznlRRPy42s9pO+pQ2o46NJd/GpqZlTMtOf6IuA64bjqebWaWd7mZuWtmZgkHfjOznHHgNzPLGQd+M7OcUURMdxkmJOl3wCMVXr4QeKqKxWkErnM+uM75MJU6vzgi9hp7sCEC/1RIujsiuqe7HPXkOueD65wPtaizUz1mZjnjwG9mljN5CPyrprsA08B1zgfXOR+qXuemz/Gbmdmu8tDiNzOzAg78ZmY509SBX9IKST+R9DNJn5ju8tSCpIskPSlpc8GxBZLWSvpp+nWP6SxjNUk6QNLNkh6Q9GNJ56XHm7nOsyXdKenetM4XpMcPknRH+vP9n5Lap7us1SapVdJGSdek75u6zpK2SLpf0iZJd6fHqv6z3bSBv2BT9zcChwJnS2rGBfkvBlaMOfYJ4KaIeClwU/q+WQwBH42IQ4FjgA+k39dmrnM/cGJEvBI4Elgh6Rjg74F/joiXAM8C752+ItbMecCDBe/zUOflEXFkwdj9qv9sN23gpw6bus8EEbEeeGbM4dOBr6evvw68pZ5lqqWIeDwi7klfbyUJCvvT3HWOiOhL37al/wI4Ebg8Pd5UdQaQtAg4FfhK+l40eZ1LqPrPdjMH/mKbuu8/TWWpt66IeDx9/VugazoLUyuSFgNHAXfQ5HVOUx6bgCeBtcDPgZ6IGEpPacaf7y8CHwdG0vd70vx1DuBGSRskrUyPVf1ne8Zutm7VEREhqenG7ErqBK4APhIRvUljMNGMdY6IYeBISfOB7wCvmN4S1Zak04AnI2KDpGXTXJx6Oj4ifiNpb2CtpIcKP6zWz3Yzt/hrvqn7DPaEpH0B0q9PTnN5qkpSG0nQ/3ZEXJkebuo6j4qIHuBm4FhgvqTRxluz/XwfB7xZ0haSNO2JwJdo7joTEb9Jvz5J8gv+1dTgZ7uZA//OTd3Tnv+zgKunuUz1cjVwbvr6XOC701iWqkrzvF8FHoyILxR81Mx13itt6SNpN+ANJH0bNwNvS09rqjpHxCcjYlFELCb5f/f7EfFOmrjOknaXNHf0NXAysJka/Gw39cxdSX9Akicc3dT9c9NbouqTdAmwjGTp1ieA84GrgEuBA0mWs357RIztAG5Iko4HbgXu54Xc79+Q5Pmbtc5HkHTqtZI01i6NiM9KOpikNbwA2Ai8KyL6p6+ktZGmej4WEac1c53Tun0nfTsL+H8R8TlJe1Lln+2mDvxmZjZeM6d6zMysCAd+M7OcceA3M8sZB34zs5xx4DczyxkHfmtokt4iKSRNOJNV0kckzZnCs/5Y0pcrvX4qJC2T9NrpeLY1Hwd+a3RnA7elXyfyEaDiwD/NlgEO/FYVDvzWsNL1eo4nWZr3rILjrZIulLRZ0n2SPiTpw8B+wM2Sbk7P6yu45m2SLk5fvyld832jpO9JKrsoVrpe+lXps25PJ1wh6TOSPlZw3mZJi9N/D0n6tqQHJV0++pdIuh77wvR1t6R16WJ07wf+Il2n/fcl/WF6v3slra/Cf07LES/SZo3sdOCGiHhY0tOSlkbEBmAlsBg4MiKGJC2IiGck/SXJWudPTXDf24Bj0gWx3keyQuRHy5x/AbAxIt4i6UTgGyTr5pfzcuC9EfEDSRcBfw5cWOzEiNgi6d+Bvoi4EEDS/cAp6YJe8yd4ltku3OK3RnY2yfR90q+j6Z6TgP87unxvBdPbFwFr0uD6V8BhE5x/PPDN9FnfB/aUNG+Ca34dET9IX38rvcdk/AC4WNJ/I1nKwSwzt/itIUlaQLJi4+HpMrWtQEj6q0ncpnC9ktkFr/8F+EJEXJ2uE/OZCos5xK6Nq8JnjF0rZfR94TWzKSEi3i/pNSQblWxI/9p5usJyWs64xW+N6m3ANyPixRGxOCIOAH4J/D7JRiV/Orp8b/pLAmArMLfgHk9I+j1JLcBbC46/iBeW+z2Xid0KvDN91jLgqYjoBbYAr0qPvwo4qOCaAyUdm74+hyS9RHrN0vT1mQXn71J2SYdExB0R8d+B37HrEuRmZTnwW6M6mxdWMhx1RXr8K8CvgPsk3UsSWAFWATeMdu6S7F16DfBD4PGC+3wGuEzSBmCi/oDR85dKug/4PC/8srgCWCDpx8AHgYcLrvkJyX7BDwJ7AP+WHr8A+JKSjbaHC87/L+Cto527wD8q2ZR7c1r+ezOU0wzw6pxmdZeO0rkmIpZMd1ksn9ziNzPLGbf4zcxyxi1+M7OcceA3M8sZB34zs5xx4DczyxkHfjOznPn/T8N1RSRlKJEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX5w42vRXMWj"
      },
      "source": [
        "### Answer 4\n",
        "\n",
        "We add 1 to the outputs before passing it through log() because log(0) is undefined and the values of x and y can be 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOUf7iOkYxcM"
      },
      "source": [
        "### Answer 5\n",
        "\n",
        "\n",
        "MSE have higher loss and error values. MAE have smaller loss and error values. MSLE have the least loss and error values. Therefore, MSLE is the loss function is the most successful in minimising error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_9nfLOffJ_P"
      },
      "source": [
        "### Answer 6\n",
        "\n",
        "MAPE have minimum error values compared to MSE. MAE have minimum error values compared to MAPE. MSLE have minimum error values compared to MAPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "outputId": "2aadd5db-14d5-4127-af5d-f9fd1d0b0d34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= 'mape',\n",
        "              metrics=['mape'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 2s 18ms/step - loss: 99.3488 - mape: 99.3488 - val_loss: 96.9757 - val_mape: 96.9757\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 94.6637 - mape: 94.6637 - val_loss: 90.7192 - val_mape: 90.7192\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 86.7076 - mape: 86.7076 - val_loss: 80.0292 - val_mape: 80.0292\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 73.3253 - mape: 73.3253 - val_loss: 64.8613 - val_mape: 64.8613\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 55.9520 - mape: 55.9520 - val_loss: 47.5200 - val_mape: 47.5200\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 39.6599 - mape: 39.6599 - val_loss: 31.8333 - val_mape: 31.8333\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 29.2288 - mape: 29.2288 - val_loss: 25.2013 - val_mape: 25.2013\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 24.0340 - mape: 24.0340 - val_loss: 19.3590 - val_mape: 19.3590\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 19.9988 - mape: 19.9988 - val_loss: 16.4801 - val_mape: 16.4801\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.3480 - mape: 17.3480 - val_loss: 17.1924 - val_mape: 17.1924\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15.5940 - mape: 15.5940 - val_loss: 15.6241 - val_mape: 15.6241\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.9825 - mape: 14.9825 - val_loss: 15.2038 - val_mape: 15.2038\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14.4790 - mape: 14.4790 - val_loss: 14.7016 - val_mape: 14.7016\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13.6823 - mape: 13.6823 - val_loss: 13.5508 - val_mape: 13.5508\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13.2004 - mape: 13.2004 - val_loss: 13.1993 - val_mape: 13.1993\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12.7919 - mape: 12.7919 - val_loss: 12.7106 - val_mape: 12.7106\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12.3188 - mape: 12.3188 - val_loss: 12.7940 - val_mape: 12.7940\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11.8592 - mape: 11.8592 - val_loss: 12.2378 - val_mape: 12.2378\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.5904 - mape: 11.5904 - val_loss: 11.6986 - val_mape: 11.6986\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.1286 - mape: 11.1286 - val_loss: 12.2320 - val_mape: 12.2320\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.9273 - mape: 10.9273 - val_loss: 11.5755 - val_mape: 11.5755\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.7881 - mape: 10.7881 - val_loss: 11.5232 - val_mape: 11.5232\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.7738 - mape: 10.7738 - val_loss: 12.5434 - val_mape: 12.5434\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.6062 - mape: 10.6062 - val_loss: 11.8466 - val_mape: 11.8466\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.7174 - mape: 10.7174 - val_loss: 10.7849 - val_mape: 10.7849\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.3682 - mape: 10.3682 - val_loss: 12.5020 - val_mape: 12.5020\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.7792 - mape: 10.7792 - val_loss: 10.9422 - val_mape: 10.9422\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.2806 - mape: 10.2806 - val_loss: 10.8165 - val_mape: 10.8165\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.0133 - mape: 10.0133 - val_loss: 11.6160 - val_mape: 11.6160\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.8812 - mape: 9.8812 - val_loss: 10.3755 - val_mape: 10.3755\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.7926 - mape: 9.7926 - val_loss: 11.1841 - val_mape: 11.1841\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.7499 - mape: 9.7499 - val_loss: 10.6891 - val_mape: 10.6891\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.7065 - mape: 9.7065 - val_loss: 10.6558 - val_mape: 10.6558\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.5707 - mape: 9.5707 - val_loss: 10.8345 - val_mape: 10.8345\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.4700 - mape: 9.4700 - val_loss: 10.8511 - val_mape: 10.8511\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3884 - mape: 9.3884 - val_loss: 10.7740 - val_mape: 10.7740\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4527 - mape: 9.4527 - val_loss: 11.3090 - val_mape: 11.3090\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4217 - mape: 9.4217 - val_loss: 11.0040 - val_mape: 11.0040\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.7079 - mape: 9.7079 - val_loss: 10.7575 - val_mape: 10.7575\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4176 - mape: 9.4176 - val_loss: 11.0190 - val_mape: 11.0190\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.3373 - mape: 9.3373 - val_loss: 10.5456 - val_mape: 10.5456\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.0386 - mape: 9.0386 - val_loss: 10.9483 - val_mape: 10.9483\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.2065 - mape: 9.2065 - val_loss: 10.8302 - val_mape: 10.8302\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.0354 - mape: 9.0354 - val_loss: 10.8664 - val_mape: 10.8664\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9616 - mape: 8.9616 - val_loss: 10.8798 - val_mape: 10.8798\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9118 - mape: 8.9118 - val_loss: 10.6328 - val_mape: 10.6328\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.0109 - mape: 9.0109 - val_loss: 11.1767 - val_mape: 11.1767\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.0780 - mape: 9.0780 - val_loss: 10.4720 - val_mape: 10.4720\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.0532 - mape: 9.0532 - val_loss: 10.3390 - val_mape: 10.3390\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9608 - mape: 8.9608 - val_loss: 10.4395 - val_mape: 10.4395\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.9108 - mape: 8.9108 - val_loss: 11.0692 - val_mape: 11.0692\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.0316 - mape: 9.0316 - val_loss: 10.9863 - val_mape: 10.9863\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.0951 - mape: 9.0951 - val_loss: 11.1421 - val_mape: 11.1421\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.6424 - mape: 8.6424 - val_loss: 10.1706 - val_mape: 10.1706\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.5840 - mape: 8.5840 - val_loss: 10.5425 - val_mape: 10.5425\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.5429 - mape: 8.5429 - val_loss: 10.6853 - val_mape: 10.6853\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.5268 - mape: 8.5268 - val_loss: 10.3080 - val_mape: 10.3080\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.4666 - mape: 8.4666 - val_loss: 11.2226 - val_mape: 11.2226\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7534 - mape: 8.7534 - val_loss: 9.9490 - val_mape: 9.9490\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8433 - mape: 8.8433 - val_loss: 10.9155 - val_mape: 10.9155\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.2999 - mape: 8.2999 - val_loss: 10.0186 - val_mape: 10.0186\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2064 - mape: 8.2064 - val_loss: 10.0939 - val_mape: 10.0939\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2364 - mape: 8.2364 - val_loss: 10.7062 - val_mape: 10.7062\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.1508 - mape: 8.1508 - val_loss: 10.2005 - val_mape: 10.2005\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2482 - mape: 8.2482 - val_loss: 9.9100 - val_mape: 9.9100\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0856 - mape: 8.0856 - val_loss: 10.1381 - val_mape: 10.1381\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2298 - mape: 8.2298 - val_loss: 10.2836 - val_mape: 10.2836\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0263 - mape: 8.0263 - val_loss: 10.1400 - val_mape: 10.1400\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9765 - mape: 7.9765 - val_loss: 10.0487 - val_mape: 10.0487\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8837 - mape: 7.8837 - val_loss: 10.4173 - val_mape: 10.4173\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.9158 - mape: 7.9158 - val_loss: 10.1460 - val_mape: 10.1460\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8444 - mape: 7.8444 - val_loss: 9.7230 - val_mape: 9.7230\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.1312 - mape: 8.1312 - val_loss: 10.2985 - val_mape: 10.2985\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0688 - mape: 8.0688 - val_loss: 10.3733 - val_mape: 10.3733\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8503 - mape: 7.8503 - val_loss: 10.4143 - val_mape: 10.4143\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.9000 - mape: 7.9000 - val_loss: 9.9426 - val_mape: 9.9426\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7574 - mape: 7.7574 - val_loss: 10.3558 - val_mape: 10.3558\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6723 - mape: 7.6723 - val_loss: 10.0275 - val_mape: 10.0275\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.6417 - mape: 7.6417 - val_loss: 9.8165 - val_mape: 9.8165\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.6824 - mape: 7.6824 - val_loss: 10.1279 - val_mape: 10.1279\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6848 - mape: 7.6848 - val_loss: 10.4146 - val_mape: 10.4146\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4697 - mape: 8.4697 - val_loss: 10.0502 - val_mape: 10.0502\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8754 - mape: 7.8754 - val_loss: 9.6227 - val_mape: 9.6227\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6598 - mape: 7.6598 - val_loss: 9.9722 - val_mape: 9.9722\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4795 - mape: 7.4795 - val_loss: 9.7436 - val_mape: 9.7436\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.5239 - mape: 7.5239 - val_loss: 9.9984 - val_mape: 9.9984\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7334 - mape: 7.7334 - val_loss: 9.4045 - val_mape: 9.4045\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4901 - mape: 7.4901 - val_loss: 10.2068 - val_mape: 10.2068\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8910 - mape: 7.8910 - val_loss: 9.6299 - val_mape: 9.6299\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3105 - mape: 7.3105 - val_loss: 10.2265 - val_mape: 10.2265\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3077 - mape: 7.3077 - val_loss: 10.0831 - val_mape: 10.0831\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.7989 - mape: 7.7989 - val_loss: 9.4830 - val_mape: 9.4830\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.3945 - mape: 7.3945 - val_loss: 9.6126 - val_mape: 9.6126\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6050 - mape: 7.6050 - val_loss: 9.9646 - val_mape: 9.9646\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4087 - mape: 7.4087 - val_loss: 9.8089 - val_mape: 9.8089\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1302 - mape: 7.1302 - val_loss: 9.4205 - val_mape: 9.4205\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1059 - mape: 7.1059 - val_loss: 9.4627 - val_mape: 9.4627\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0349 - mape: 7.0349 - val_loss: 9.5777 - val_mape: 9.5777\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9735 - mape: 6.9735 - val_loss: 9.5856 - val_mape: 9.5856\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.8667 - mape: 6.8667 - val_loss: 9.2926 - val_mape: 9.2926\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0832 - mape: 7.0832 - val_loss: 9.6332 - val_mape: 9.6332\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2217 - mape: 7.2217 - val_loss: 10.1065 - val_mape: 10.1065\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1735 - mape: 7.1735 - val_loss: 10.1803 - val_mape: 10.1803\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3362 - mape: 7.3362 - val_loss: 9.5091 - val_mape: 9.5091\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0481 - mape: 7.0481 - val_loss: 9.7382 - val_mape: 9.7382\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.0604 - mape: 7.0604 - val_loss: 9.5394 - val_mape: 9.5394\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0870 - mape: 7.0870 - val_loss: 9.3456 - val_mape: 9.3456\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9706 - mape: 6.9706 - val_loss: 9.7973 - val_mape: 9.7973\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.7998 - mape: 6.7998 - val_loss: 9.3914 - val_mape: 9.3914\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8747 - mape: 6.8747 - val_loss: 9.3733 - val_mape: 9.3733\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.7644 - mape: 6.7644 - val_loss: 9.5894 - val_mape: 9.5894\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.7961 - mape: 6.7961 - val_loss: 9.4197 - val_mape: 9.4197\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8093 - mape: 6.8093 - val_loss: 9.0986 - val_mape: 9.0986\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8112 - mape: 6.8112 - val_loss: 8.9676 - val_mape: 8.9676\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0058 - mape: 7.0058 - val_loss: 9.5663 - val_mape: 9.5663\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.6309 - mape: 6.6309 - val_loss: 9.4966 - val_mape: 9.4966\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.6107 - mape: 6.6107 - val_loss: 9.4909 - val_mape: 9.4909\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.6783 - mape: 6.6783 - val_loss: 9.4723 - val_mape: 9.4723\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.6962 - mape: 6.6962 - val_loss: 9.2243 - val_mape: 9.2243\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.5676 - mape: 6.5676 - val_loss: 9.3775 - val_mape: 9.3775\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.5358 - mape: 6.5358 - val_loss: 8.8684 - val_mape: 8.8684\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.8573 - mape: 6.8573 - val_loss: 9.1317 - val_mape: 9.1317\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.5315 - mape: 6.5315 - val_loss: 9.7655 - val_mape: 9.7655\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.8769 - mape: 6.8769 - val_loss: 9.6720 - val_mape: 9.6720\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 7.1928 - mape: 7.1928 - val_loss: 9.5745 - val_mape: 9.5745\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.8301 - mape: 6.8301 - val_loss: 9.5213 - val_mape: 9.5213\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.5493 - mape: 6.5493 - val_loss: 9.1160 - val_mape: 9.1160\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.5342 - mape: 6.5342 - val_loss: 9.1795 - val_mape: 9.1795\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.4812 - mape: 6.4812 - val_loss: 9.5810 - val_mape: 9.5810\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.4114 - mape: 6.4114 - val_loss: 9.3941 - val_mape: 9.3941\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.4005 - mape: 6.4005 - val_loss: 9.1598 - val_mape: 9.1598\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.5374 - mape: 6.5374 - val_loss: 9.5885 - val_mape: 9.5885\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.9713 - mape: 6.9713 - val_loss: 9.1656 - val_mape: 9.1656\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.6773 - mape: 6.6773 - val_loss: 9.1459 - val_mape: 9.1459\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.5576 - mape: 6.5576 - val_loss: 9.3148 - val_mape: 9.3148\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.5695 - mape: 6.5695 - val_loss: 9.6941 - val_mape: 9.6941\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.4700 - mape: 6.4700 - val_loss: 9.6177 - val_mape: 9.6177\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4080 - mape: 6.4080 - val_loss: 9.4429 - val_mape: 9.4429\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5861 - mape: 6.5861 - val_loss: 9.1678 - val_mape: 9.1678\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2360 - mape: 6.2360 - val_loss: 9.2137 - val_mape: 9.2137\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3920 - mape: 6.3920 - val_loss: 9.3738 - val_mape: 9.3738\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4552 - mape: 6.4552 - val_loss: 9.1614 - val_mape: 9.1614\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2546 - mape: 6.2546 - val_loss: 9.3949 - val_mape: 9.3949\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.4583 - mape: 6.4583 - val_loss: 8.8829 - val_mape: 8.8829\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.3629 - mape: 6.3629 - val_loss: 9.1812 - val_mape: 9.1812\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.0766 - mape: 6.0766 - val_loss: 9.2162 - val_mape: 9.2162\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1476 - mape: 6.1476 - val_loss: 9.1168 - val_mape: 9.1168\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2240 - mape: 6.2240 - val_loss: 9.4492 - val_mape: 9.4492\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1600 - mape: 6.1600 - val_loss: 8.8873 - val_mape: 8.8873\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0633 - mape: 6.0633 - val_loss: 9.1569 - val_mape: 9.1569\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.1538 - mape: 6.1538 - val_loss: 9.1039 - val_mape: 9.1039\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.2384 - mape: 6.2384 - val_loss: 9.0453 - val_mape: 9.0453\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.5850 - mape: 6.5850 - val_loss: 9.1471 - val_mape: 9.1471\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2978 - mape: 6.2978 - val_loss: 9.3347 - val_mape: 9.3347\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3038 - mape: 6.3038 - val_loss: 9.1957 - val_mape: 9.1957\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.1116 - mape: 6.1116 - val_loss: 9.1605 - val_mape: 9.1605\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1428 - mape: 6.1428 - val_loss: 9.0081 - val_mape: 9.0081\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.6444 - mape: 6.6444 - val_loss: 9.2356 - val_mape: 9.2356\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4287 - mape: 6.4287 - val_loss: 9.3286 - val_mape: 9.3286\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1750 - mape: 6.1750 - val_loss: 9.1871 - val_mape: 9.1871\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9684 - mape: 5.9684 - val_loss: 9.2441 - val_mape: 9.2441\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1186 - mape: 6.1186 - val_loss: 9.2339 - val_mape: 9.2339\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.1129 - mape: 6.1129 - val_loss: 8.8803 - val_mape: 8.8803\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.1236 - mape: 6.1236 - val_loss: 8.7792 - val_mape: 8.7792\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3342 - mape: 6.3342 - val_loss: 9.4841 - val_mape: 9.4841\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.4381 - mape: 6.4381 - val_loss: 9.3258 - val_mape: 9.3258\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8384 - mape: 5.8384 - val_loss: 9.1019 - val_mape: 9.1019\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.0059 - mape: 6.0059 - val_loss: 9.0570 - val_mape: 9.0570\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.9443 - mape: 5.9443 - val_loss: 8.7439 - val_mape: 8.7439\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0091 - mape: 6.0091 - val_loss: 8.7204 - val_mape: 8.7204\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9588 - mape: 5.9588 - val_loss: 9.2327 - val_mape: 9.2327\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.0489 - mape: 6.0489 - val_loss: 9.0407 - val_mape: 9.0407\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9026 - mape: 5.9026 - val_loss: 8.8805 - val_mape: 8.8805\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8847 - mape: 5.8847 - val_loss: 9.0711 - val_mape: 9.0711\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9561 - mape: 5.9561 - val_loss: 8.9819 - val_mape: 8.9819\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9077 - mape: 5.9077 - val_loss: 8.9725 - val_mape: 8.9725\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.8893 - mape: 5.8893 - val_loss: 8.9988 - val_mape: 8.9988\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8255 - mape: 5.8255 - val_loss: 8.9447 - val_mape: 8.9447\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.7617 - mape: 5.7617 - val_loss: 9.0155 - val_mape: 9.0155\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0873 - mape: 6.0873 - val_loss: 8.6556 - val_mape: 8.6556\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.8159 - mape: 5.8159 - val_loss: 8.8058 - val_mape: 8.8058\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8309 - mape: 5.8309 - val_loss: 8.7216 - val_mape: 8.7216\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.7247 - mape: 5.7247 - val_loss: 8.7547 - val_mape: 8.7547\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.9123 - mape: 5.9123 - val_loss: 8.9122 - val_mape: 8.9122\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8531 - mape: 5.8531 - val_loss: 9.3099 - val_mape: 9.3099\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.9022 - mape: 5.9022 - val_loss: 8.9431 - val_mape: 8.9431\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.6400 - mape: 5.6400 - val_loss: 8.7966 - val_mape: 8.7966\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7008 - mape: 5.7008 - val_loss: 8.7931 - val_mape: 8.7931\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8889 - mape: 5.8889 - val_loss: 9.2285 - val_mape: 9.2285\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7526 - mape: 5.7526 - val_loss: 8.8194 - val_mape: 8.8194\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.6609 - mape: 5.6609 - val_loss: 8.8190 - val_mape: 8.8190\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.9654 - mape: 5.9654 - val_loss: 8.7215 - val_mape: 8.7215\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5885 - mape: 6.5885 - val_loss: 9.4380 - val_mape: 9.4380\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.2206 - mape: 6.2206 - val_loss: 9.2549 - val_mape: 9.2549\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0360 - mape: 6.0360 - val_loss: 8.4757 - val_mape: 8.4757\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2688 - mape: 6.2688 - val_loss: 9.5872 - val_mape: 9.5872\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8411 - mape: 5.8411 - val_loss: 9.7617 - val_mape: 9.7617\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0813 - mape: 6.0813 - val_loss: 9.0334 - val_mape: 9.0334\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.1010 - mape: 6.1010 - val_loss: 8.9377 - val_mape: 8.9377\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7182 - mape: 5.7182 - val_loss: 8.7783 - val_mape: 8.7783\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.5259 - mape: 5.5259 - val_loss: 8.9574 - val_mape: 8.9574\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.6846 - mape: 5.6846 - val_loss: 9.1582 - val_mape: 9.1582\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.6610 - mape: 5.6610 - val_loss: 8.6421 - val_mape: 8.6421\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5397 - mape: 5.5397 - val_loss: 8.8468 - val_mape: 8.8468\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7141 - mape: 5.7141 - val_loss: 8.8297 - val_mape: 8.8297\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.5147 - mape: 5.5147 - val_loss: 8.8509 - val_mape: 8.8509\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.5516 - mape: 5.5516 - val_loss: 9.0741 - val_mape: 9.0741\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.7199 - mape: 5.7199 - val_loss: 8.5562 - val_mape: 8.5562\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5213 - mape: 5.5213 - val_loss: 9.2442 - val_mape: 9.2442\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5846 - mape: 5.5846 - val_loss: 8.7600 - val_mape: 8.7600\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.5313 - mape: 5.5313 - val_loss: 8.8622 - val_mape: 8.8622\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.4660 - mape: 5.4660 - val_loss: 8.6780 - val_mape: 8.6780\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.6437 - mape: 5.6437 - val_loss: 8.5997 - val_mape: 8.5997\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0103 - mape: 6.0103 - val_loss: 9.2200 - val_mape: 9.2200\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9029 - mape: 5.9029 - val_loss: 9.5381 - val_mape: 9.5381\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8879 - mape: 5.8879 - val_loss: 8.8898 - val_mape: 8.8898\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.5909 - mape: 5.5909 - val_loss: 8.9429 - val_mape: 8.9429\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.4561 - mape: 5.4561 - val_loss: 8.6735 - val_mape: 8.6735\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.7596 - mape: 5.7596 - val_loss: 8.8834 - val_mape: 8.8834\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.3517 - mape: 5.3517 - val_loss: 8.7328 - val_mape: 8.7328\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.4487 - mape: 5.4487 - val_loss: 8.4745 - val_mape: 8.4745\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5.3240 - mape: 5.3240 - val_loss: 8.7211 - val_mape: 8.7211\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.4608 - mape: 5.4608 - val_loss: 9.0500 - val_mape: 9.0500\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.3059 - mape: 5.3059 - val_loss: 8.7140 - val_mape: 8.7140\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.3534 - mape: 5.3534 - val_loss: 8.8817 - val_mape: 8.8817\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5252 - mape: 5.5252 - val_loss: 9.5067 - val_mape: 9.5067\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5177 - mape: 5.5177 - val_loss: 9.2651 - val_mape: 9.2651\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9532 - mape: 5.9532 - val_loss: 9.6635 - val_mape: 9.6635\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.4648 - mape: 5.4648 - val_loss: 8.5686 - val_mape: 8.5686\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.4970 - mape: 5.4970 - val_loss: 8.7822 - val_mape: 8.7822\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5285 - mape: 5.5285 - val_loss: 8.6099 - val_mape: 8.6099\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.5273 - mape: 5.5273 - val_loss: 8.5074 - val_mape: 8.5074\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2324 - mape: 6.2324 - val_loss: 9.3718 - val_mape: 9.3718\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7509 - mape: 5.7509 - val_loss: 8.6734 - val_mape: 8.6734\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7446 - mape: 5.7446 - val_loss: 9.3704 - val_mape: 9.3704\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.6972 - mape: 5.6972 - val_loss: 9.1970 - val_mape: 9.1970\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.3908 - mape: 5.3908 - val_loss: 8.9062 - val_mape: 8.9062\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.3308 - mape: 5.3308 - val_loss: 8.7738 - val_mape: 8.7738\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.4889 - mape: 5.4889 - val_loss: 9.0652 - val_mape: 9.0652\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.4371 - mape: 5.4371 - val_loss: 8.3820 - val_mape: 8.3820\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.4119 - mape: 5.4119 - val_loss: 8.4718 - val_mape: 8.4718\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.3355 - mape: 5.3355 - val_loss: 8.8464 - val_mape: 8.8464\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.0685 - mape: 5.0685 - val_loss: 8.5094 - val_mape: 8.5094\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.2104 - mape: 5.2104 - val_loss: 8.6268 - val_mape: 8.6268\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.1537 - mape: 5.1537 - val_loss: 9.0339 - val_mape: 9.0339\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.2642 - mape: 5.2642 - val_loss: 8.8787 - val_mape: 8.8787\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.1190 - mape: 5.1190 - val_loss: 8.6169 - val_mape: 8.6169\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.1941 - mape: 5.1941 - val_loss: 8.7284 - val_mape: 8.7284\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.3383 - mape: 5.3383 - val_loss: 8.9221 - val_mape: 8.9221\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.2396 - mape: 5.2396 - val_loss: 9.2706 - val_mape: 9.2706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f514eb36070>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 5 Colab Notebook to your Github repository under \"Day 5\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}